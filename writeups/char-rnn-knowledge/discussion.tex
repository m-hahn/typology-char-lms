\section{Discussion}
\label{sec:discussion}

We probed the linguistic information induced by a character-level LSTM
language model trained on unsegmented text. The model was found to
possess implicit knowledge about phonotactics, word units,
morphosyntactic classes, syntactic agreement phenomena and basic
semantics. A more standard model pre-initialized with a word
vocabulary and reading tokenized input was in general superior on the
higher-level tasks, but the performance of our agnostic model did not
generally lag much behind, suggesting that the word prior is helpful
but not fundamental. The performance of a character-level RNN was less
consistent than that of the LSTM, suggesting that the ability of the
latter to track information across longer time stretches is important
to extract the correct linguistic generalizations. N-gram baselines
relying on adjacent-string statistics failed almost everywhere,
showing that the neural models are tapping into somewhat deeper
linguistic templates.

Our results are preliminary in many ways. The tests we used are
generally quite simple \cite[we did not attempt, for example, to model
long-distance subject-verb agreement, a task that is challenging even
for word-based models:][]{Linzen:etal:2016}. Still, they do suggest
that a large corpus, combined with the very weak priors encoded in an
LSTM, might suffice to support genuine linguistic generalizations. In
future work, we would like to check if stronger priors are needed when
learning from smaller amounts of training data.

Unlike standard word-level models, CNLMs lack a word-based
lexicon. Any information they might acquire about units larger than
characters must be stored in their recurrent weights. Given that
nearly all contemporary linguistics recognizes a central role to the
lexicon \cite[see,
e.g.,][]{Sag:etal:2003,Goldberg:2005,Radford:2006,Bresnan:etal:2016,Jezek:2016},
in future work we would like to explore how lexical knowledge is
implicitly encoded in the distributed memory of our CNLMs.

One of our original motivations for not assuming word primitives is that a rigid word notion is problematic both cross-linguistically (cf.~polysynthetic and agglutinative languages) and when analyzing a single language (cf.~the common  view  that the lexicon hosts units at different levels of the linguistic hierarchy, from  morphemes to large syntactic constructions). Our brief analysis of the CNLM over- and undersegmentations suggested that it is indeed capable to flexibly store information about units at different levels. However, this topic  remained largely unexplored, and we plan to systematically tackle it in future work.

