\section{Discussion}
\label{sec:discussion}

We probed the linguistic information induced by a character-level LSTM
language model trained on unsegmented text. The model was found to
possess implicit knowledge about a range of intuitively word-mediated
phenomena, such as sensitivity to lexical categories and syntactic and
shallow-semantics dependencies. A model initialized with a word
vocabulary and fed tokenized input, was in general superior, but the
performance of the word-less model did not lag much behind, suggesting
that word priors are helpful but not strictly required. A
character-level RNN was less consistent than the LSTM, suggesting that
the latter ability to track information across longer time
spans is important to make the correct generalizations. The
character-level models consistently outperformed n-gram controls,
confirming they are tapping into more abstract patterns than local
co-occurrence statistics. As a first step towards understanding
\emph{how} character-level models handle supra-character phenomena, we
searched and found specialized boundary-tracking units in them. These
units are not only and not always sensitive to word boundaries, but
also respond to other salient items, such as morphemes and multi-word
expressions, in accordance with an ``emergent'' and flexible view of
the basic constituents of language \cite{Schiering:etal:2010}. Our
results are preliminary in many ways. Out tests are relatively
simple. We did not attempt, for example, to model long-distance
agreement in presence of distractors, a challenging task even for
word-based models and humans \citep{Gulordava:etal:2018}. The results
on number classification in German suggest that the models might not
be capturing linguistic generalizations of the correct degree of
abstractness, settling for shallower heuristics. Still, as a whole,
our work suggests that a large corpus, combined with the weak priors
encoded in an LSTM, might suffice to learn generalizations about
word-mediated linguistic processes without a hard-coded word lexicon
or explicit wordhood cues.

Nearly all contemporary linguistics recognizes a central role to the
lexicon \cite[see, e.g.,][for very different
perspectives]{Sag:etal:2003,Goldberg:2005,Radford:2006,Bresnan:etal:2016,Jezek:2016}. Linguistic
formalisms assume that the lexicon is essentially a dictionary
of words, possibly complemented by other units, not unlike the list of
words and associated embeddings in a standard word-based
NLM. Intriguingly, our CNLMs captured a range of lexical phenomena
\emph{without} anything resembling a word dictionary. Any information
a CNLM might acquire about units larger than characters must be stored
in its recurrent weights. This suggests a radically different view of
lexical knowledge as implicitly encoded in a distributed memory, that
we intend to characterize more precisely and test in future work.

Concerning the model input, we would like to study to what extent the
linguistic abilities of the CNLM depend on the huge amount of
data it sees.  Will the word prior become more important when learning
from smaller corpora? In terms of comparison with human learning, the
Wikipedia text our CNLMs receive as input is far from what children
acquiring a language would hear. Future work should explore
character/phoneme-level learning from corpora of child-directed
speech. Still, arguably, by feeding our networks long-winded grown-up
prose, we are making the job of identifying basic constituents harder
than it might be when processing the short utterances characterizing
early child-directed speech \cite{Tomasello:2003}.

Finally, as we discussed, a rigid notion of word is problematic both
cross-linguistically (cf.~polysynthetic and agglutinative languages)
and within a single linguistic system \cite[cf.~the common view that
the lexicon hosts units at different levels of the linguistic
hierarchy, from morphemes to large syntactic constructions,
e.g.,][]{Jackendoff:1997,Croft:Cruse:2004,Goldberg:2005}. In this
perspective, the current paper provides a necessary preliminary check
that a word-free model can account for phenomena traditionally
regarded as word-based. However, future work should test whether such
model can also account for grammatical patterns that are harder to
capture in word-based formalisms, looking both at a typologically
wider range of language and at a broader range of grammatical tests.



