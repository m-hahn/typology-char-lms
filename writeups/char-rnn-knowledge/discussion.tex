\section{Discussion}
\label{sec:discussion}

We probed the linguistic information induced by a character-level LSTM
language model trained on unsegmented text. The model was found to
possess implicit knowledge about phonotactics, word units,
morphological features, syntactic agreement phenomena and basic
semantics. A more standard model pre-initialized with a word
vocabulary and reading tokenized input was in general superior on the
higher-level tasks, but the performance of our agnostic model did not
generally lag much behind, suggesting that the word prior is helpful
but not fundamental. The performance of a character-level RNN was less
consistent than that of the LSTM, suggesting that the ability of the
latter to track information across longer time stretches is important
to extract the correct linguistic generalizations. N-gram baselines
relying on adjacent-string statistics failed almost everywhere,
showing that the neural models are tapping into somewhat deeper
linguistic templates.

Our results are preliminary in many ways. The tests we used are
generally quite simple \cite[we did not attempt, for example, to model
long-distance agreement in presence of distractors, a
challenging task even for word-based models and
humans:][]{Gulordava:etal:2018}. Still, they  suggest that a large
corpus, combined with the weak priors encoded in an LSTM, might
suffice to support genuine linguistic generalizations. In future work,
we will check if stronger priors are needed when learning
from smaller amounts of training data.

Of course, the written Wikipedia text our RNNs receive as input is
very far from what children hear, and future work should explore
character/phoneme-level learning from corpora of child-directed
speech. Still, arguably, by feeding the RNNs directly long-winded
grown-up prose, we are making the job of identifying basic
constituents harder than it might be from the short utterances
characterizing early child-directed speech \cite{Tomasello:2003}.

Unlike standard word-level models, CNLMs lack a word-based
lexicon. Any information they might acquire about units larger than
characters must be stored in their recurrent weights. Given that
nearly all contemporary linguistics recognizes a central role to the
lexicon \cite[see,
e.g.,][]{Sag:etal:2003,Goldberg:2005,Radford:2006,Bresnan:etal:2016,Jezek:2016},
in future work we would like to explore how lexical knowledge is
implicitly encoded in the distributed memory of the CNLMs.

One of our original motivations for not assuming word primitives is that a rigid word notion is problematic both cross-linguistically (cf.~polysynthetic and agglutinative languages) and within a single linguistic system (cf.~the common  view  that the lexicon hosts units at different levels of the linguistic hierarchy, from  morphemes to large syntactic constructions). Our brief analysis of the CNLM over- and undersegmentations suggested that it is indeed capable to flexibly store information about units at different levels. However, this topic  remained largely unexplored, and we plan to tackle it in future work.



