\section{Experiments}
\label{sec:experiments}

\subsection{Discovering phonotactic constraints}
\label{sec:phonotactics}

We focus on German and Italian, as they have reasonably transparent orthographies.


We construct pairs of letter bigrams (corresponding to phoneme
bigrams) beginning with the same letter, such that one is
phonotactically acceptable in the language and the other isn't, but
the independent unigram probability of the unacceptable bigram is
higher than that of the acceptable one. E.g., ``\emph{br}'' is
acceptable Italian sequence, ``\emph{bt}'' isn't, although
\emph{``t''} is more frequent than \emph{``r''}. We re-train the CNLM on a version of
the training partition from which all sequences containing either bigram have been removed. We then look at
the likelihood the model assigns to both sequences. If the model
assigns a larger probability to the correct sequence, it means that it
implicitly possesses a notion of phonological categories such as
stops and sonorants, which allows it to correctly generalize from
attested (e.g., ``\emph{tr}'') sequences to unattested ones
(\emph{``br''}).

%We constructed bigram pairs where the second element was a vowel or a sonorant in the valid bigram, randomly choosing among possible vowels or sonorants if there were several.

In both languages, we constructed two groups of bigrams:
In one, the valid bigram had a vowel following a consonant; in the other, a consonant was followed by a sonorant.
In the invalid bigram, the consonant was followed by a stop or nasal.
For each onset consonant and each of the two types, we considered valid vowels or sonorants satisfying the constraint on unigram frequencies, and randomly selected one if there were several.


Due to computational constraints, we ran these models only for 5 epochs.

LSTM vs RNN

Results are shown in \ref{tab:phonotactics-results}.
Valid bigrams are assigned higher probability in all but two cases.


Discussion: note that generalization of model are purely
distributional, with no aid from perceptual or articulatory cues.



\begin{table}[t]
  \begin{center}
    \begin{tabular}{ll|cc}
      \multicolumn{2}{c}{\emph{bigrams}}&\emph{LSTM}&\emph{RNN}\\
      \hline
      \multicolumn{4}{c}{\emph{German}}\\
      \hline
           bu &  bt &  \textbf{ 4.6} &  0.22     \\
           do &  dd &  \textbf{ 1.9} &  0.05     \\
           fu &  ft &  \textbf{ 6.5} &  0.03     \\
           po &  pt &  \textbf{ 6.4} &  0.10     \\
           tu &  tt &  \textbf{ 5.4} &  0.02     \\
           zu &  zt &  \textbf{ 2.4} &  0.17     \\ \hline
           bl &  bd &   0.8          & 0.18       \\
           fl &  fd &  \textbf{ 2.1} & 0.82      \\
           fr &  fn &  \textbf{ 2.7} & 0.10      \\
           kl &  kt &  \textbf{ 3.8} & 0.10      \\
           pl &  pt &  \textbf{ 2.5} & 0.86      \\

      \hline
      \multicolumn{4}{c}{\emph{Italian}}\\
      \hline
	    bu & bd & \textbf{ 1.001} & 6e-5 \\
	    du & dt & \textbf{ 1.3} & 0.008 \\
	    fu & ft & \textbf{ 30.5} & 0.01 \\
	    pu & pt & \textbf{ 6.8} & 0.008 \\
	    tu & td &  0.2 & 3e-5 \\
	    vu & vd & \textbf{ 2.0} & 2e-5 \\
	    zu & zt & \textbf{ 55.7} & 0.01 \\ \hline
	    br & bt & \textbf{ 1.001}  &  0.006 \\
	    dr & dt & \textbf{ 2.5} & 0.4 \\
	    fr & ft & \textbf{ 2.9} & 0.001 \\
	    pr & pt & \textbf{ 5.0} & 0.008 \\
    \end{tabular}
  \end{center}
  \caption{\label{tab:phonotactics-results} Likelihood ratio between acceptable and unacceptable bigrams,    with values $>1$ in bold.}
\end{table}


\input{segmentation}

\subsection{Discovering morphological categories}
\label{sec:categories}

Focusing on German and Italian given massive morphosyntactic ambiguity
and impoverished morphology of English. Note that these are lexical
properties, probed in a model that has no explicit notion of word!

\paragraph{Word classes (nouns vs.~verbs)}


We sampled 500 verbs and 500 nouns from the training set, each with the requirement that they end in -en (German) or -re (Italian).
We did this since these final syllables are common among both verbs and nouns, setting the bar higher for methods relying on surface cues.
We then recorded the hidden states of the CNLM after reading each of these words.
We randomly selected N=20 training examples, balanced between the two POS classes, to create a logistic classifier distinguishing nouns and verbs from the hidden states, and tested on the remaining examples.
We repeated this experiment 100 times to control for variation between the random train-test splits.

As baselines, we considered the autoencoder and word embeddings from the output layer of the word-based language model.
We further ran the RNN CNLM.

Results are shown in Table~\ref{tab:pos-results}.
Outperforming autoencoder shows that model has learned categories based on broader distributional evidence, not just typical strings cueing nouns and verbs.


We then varied N from 2 to 100.
Resulting accuracies for German are shown in Figure~\ref{fig:pos-induction}; they confirm that the CNLM-based encodings distinguish the categories well for small training sets, while the autoencoder does not catch up even with 100 training examples.


%Baselines: autoencoder, word-based NLM embeddings, also LSTM vs
%RNN.


\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l|l}
      \multicolumn{1}{c}{}&\emph{German}&\emph{Italian}\\
      \hline
      LSTM & 89.0 & 94.0 \\
      RNN & 82.0 & 91.9 \\
      Autoencoder & 65.1 & 82.8 \\
      WordNLM & 97.4 & 96.0 \\
    \end{tabular}
  \end{center}
  \caption{\label{tab:pos-results} POS accuracy (20 training examples per category)}
\end{table}

%Report CNLM vs baseline comparison in function of training examples as in the Quip (possibly for a language only).

\begin{figure}
\includegraphics[width=0.48\textwidth]{figures/german_pos_nouns_verbs.png}
	\caption{POS accuracy as a function of training examples for the LSTM (blue) and the autoencoder (orange) in German.}\label{fig:pos-induction}
\end{figure}





\paragraph{Number}

Does the hidden state of the CNLM store an abstract notion of
number. German nouns can be binned in different classes depending on
the morpheme or morphological process they use to form the plural. We
train a number classifier on a subset of these classes, test on the
others: if model generalizes correctly, it means that it knows about
number independently of its surface expression.

Specifics of data-set construction and classifier training. Pick only one setup, e.g., training on -n, -s, and -e.

Baselines/comparisons: LSTM vs RNN, autoencoder, word-based NLM.

Results could be summarized as in Table \ref{tab:number-results}.


\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l|l|l}
      \multicolumn{1}{c}{}&\emph{training plurals}&\emph{-r}&\emph{no-suffix}\\
      \hline
      LSTM&\ldots&\ldots\\
      RNN&\ldots&\ldots&\ldots\\
      Autoencoder&\ldots&\ldots&\ldots\\
      WordNLM&\ldots&\ldots&\ldots\\
    \end{tabular}
  \end{center}
  \caption{\label{tab:number-results} Figure of merit is accuracy for plural prediction.}
\end{table}


Control follow-up: singular nouns with plural ending (with similar table?).

\subsection{Capturing syntactic dependencies}
\label{sec:dependencies}

Despite not having pre-defined information about words and morphemes,
is the model able to capture non-adjacent syntactic dependencies? NB:
actually for a CNLM even \emph{``\textbf{la} bell\textbf{a}''} is long
distance! Constructions will be language-specific, so we discuss
German and Italian separately (not much in English).

As usual, specifics of training etc that depart from general setup.

\paragraph{German} We consider 4 constructions:
\begin{inparaenum}[i)]
\item article-noun gender agreement, possibly with material in the middle,
\item determiner-noun case concord, again with material in the middle,
\item preposition case sub-categorization, with material in the middle.
\end{inparaenum}


\paragraph{Gender Agreement}
Each German noun belongs to one of three genders (masculine, feminine, neuter), which are morphologically marked on the article.
As the article and the noun can be separated by adjectives and adverbs, we can probe not only the CNLM's knowledge of nouns' genders, but also its ability to model gender agreement across distances.
We create stimuli such as
\begin{tabular}{lllllll}
	\{der, die, das\}& sehr& extrem& rote& Baum \\
	article & adverb & adverb & adjective & noun \\
	the & very & extremely & red & tree
\end{tabular}
where the correct nominative singular article matches the gender of the noun.
We select all nominative singular nouns from the German Universal Dependencies treebank \cite{de2006generating,mcdonald2013universal}, and furthermore all adjectives from the training set.
We construct four conditions varying the number of words between the article and the noun:
We first consider stimuli where no material intervenes.\footnote{Due to syncretism in the article paradigm, there sometimes is ambiguity in the choice of the correct article if the noun's morphology does not uniquely indicate that it is nominative singular. As this affects all feminine nouns, we did not remove such cases. Importantly, this issue is solved as soon as an adjective is present, as their form uniquely indicates that the phrase is nominative singular.}
In the second condition, a randomly selected adjective with the correct (nominative singular) case ending is added.
Crucially, the ending of the adjective does not reveal the gender of the noun.
In the third and fourth condition, one (sehr) or two adverbs (sehr extrem) intervened between the article and the adjective.

In Figure~\ref{fig:gender}, we report accuracy for nouns from each of the three genders and averaged over the genders.
Across genders and conditions, the word-level LSTM tends to perform best, followed by te CNLM.
While the n-gram baseline performs similarly to the CNLM when there is no intervening material, accuracy drops to the random baseline (0.33) in the presence of an adjective.
This can partly be attributed to our choice to choose the adjective randomly from the vocabulary.
Note that this would not be mitigated by models that include interplation with or backoff to lower-order n-grams, as the relevant gender information is present only on the first and last word of each stimulus.
The RNN CNLM is weaker across conditions, and its accuracy drops to random as more intervening material is present.

\begin{figure*}
\includegraphics[width=0.24\textwidth]{figures/german-gender-m.pdf}
\includegraphics[width=0.24\textwidth]{figures/german-gender-f.pdf}
\includegraphics[width=0.24\textwidth]{figures/german-gender-n.pdf}
\includegraphics[width=0.24\textwidth]{figures/german-gender-total.pdf}
\caption{Accuracy on the Gender task.}\label{fig:gender}
\end{figure*}


\paragraph{Case Agreement}
Here we test the model's knowledge of case agreement between articles and nouns.
We selected the two determiners dem (dative) and des (genitive), and all nouns  of the appropriate genders that are only compatible with one of the two articles.
Adjectives and adverbs were chosen in the four conditions as in the gender agreement experiment.

Results are shown in Figure~\ref{fig:case}.
Again, the Word LSTM has the strongest overall performance, but the CNLM is competitive as more elements intervene. Accuracy stays well above 80 \% even as three words intervene.
The n-gram model performs well if there is no intervening material, and at chance otherwise.
Accuracy of the RNN CNLM remains above chance for one or two intervening elements, but drops considerably.

Considering the results for the dative and genitive separately, accuracy slightly increases in the dative case and decreases in the genitive case.
This can be attributed to the higher baseline frequency of dative in German, suggesting that both word- and character-based networks are impacted by unigram frequencies as more words intervene.
This effect is far more pronounced for the RNN CNLM, explaining its overall decrease to chance level.
\begin{figure*}
\includegraphics[width=0.24\textwidth]{figures/german-case-Dative.pdf}
\includegraphics[width=0.24\textwidth]{figures/german-case-Genitive.pdf}
\includegraphics[width=0.24\textwidth]{figures/german-case-total.pdf}
\caption{Accuracy on the Case task.}\label{fig:case}
\end{figure*}

\paragraph{Case Subcategorization}
German verbs and prepositions lexically specify the case appropriate to their objects (mostly dative or accusative).
We probe the CNLM's knowledge of such generalization by considering the preposition \textit{mit} `with', which selects for a dative object.

To focus on knowledge of subcategorization (as opposed to inflectional paradigms), we construct objects whose head noun is a nominalized adjective, whose inflection is very regular.
We take all adjectives that occur at least 100 times in the training data, excluding those that end in -r, as these often reflect lemmatization problems.

We then selected all sentences containing a `mit' prepositional phrase in the German Universal Dependencies treebank, subject to the constraints that (1) the object is not a pronoun, and (2) the object is continuous.
For each sentence, we remove the prepositional phrase and replace it by a phrase of the form
\begin{tabular}{lllllll}
	mit & der & sehr& extrem& \{rote, roten\} \\
	prep & article & adverb & adverb & adjective \\
	with & the & very & extremely & red 
\end{tabular}
where only the -en version of the adjective is compatible with the case requirement of the preposition.
We construct three conditions by varying the presence and number of adverbs, as before.
Note that this correct version is longer than the incorrect one; this ensures that the bias for shorter sequences works against the model.
As a control for baseline probabilities, we also created control stimuli where all words up to the preposition were removed.


We conduct three evaluations: (1) accuracy, (2) accuracy minus accuracy in the control setting, (3) accuracy when classifying based on the log-likelihood ratios between full and control sentences.

\begin{figure*}
\includegraphics[width=0.24\textwidth]{figures/german-prep-Accuracy.pdf}
\includegraphics[width=0.24\textwidth]{figures/german-prep-Control.pdf}
\caption{Accuracy on the Case task.}\label{fig:prep}
\end{figure*}

Results are shown in Figure~\ref{fig:prep}.
The CNLM slightly outperforms the word-level language model.
Neither model shows accuracy decay as the number of adverbs increases.
As before, the n-gram model drops to chance as adverbs intervene, while the RNN CNLM starts with low accuracy that decays below chance.


%Discussion case-by-case, including how do we control for possible
%n-gram effects and length. N-gram control: based on counts of each
%stimulus sequence in the corpus.
%
%How to present results: figures, with number of intervening words
%(from 0 to 2 or 3) on x axis, accuracy on y axis. Multiple figures
%when different genders or cases are tested. Within the figures, one
%line per model: LSTM, RNN, n-gram, wordNLM.

\paragraph{Italian} We consider further constructions from Italian,
that confirm the results we got in German. Interesting because we look
at subset of Italian morphology where gender and number are explicitly
encoded while allowing for tightly controlled comparison of
same-length strings, limited to stimuli unseen in training corpus.
\begin{inparaenum}[i)]
\item article-noun gender agreement with material in the middle,
\item article-adjective gender agreement, with an adverb in the middle,
\item article-adjective number agreement, with an adverb in the middle.
\end{inparaenum}

\paragraph{Article-Noun Gender Agreement}
(1) eadj-aonoun: adjective ending in -e + noun that has both a masculine (-o) and feminine (-a) version.

we created \{il, la\} ADJ NOUNo, and \{il, la\} ADJ NOUNa

None of the pairs appear in the training data, and all single words appear at least 100 times.
Further, the nouns in -a and  -o have reasonably balanced frequency (neither form is twice more frequent than the other), or they are both frequent (appear at least 500 times)
Also, we considered only -e adjectives that occur at least with 10 different nouns in the prenominal position, as this is somewhat marked in Italian/


\paragraph{Article-Adjective Gender Agreement}
(2) adv-aoadj: il ADVERB \{ADJo, ADJa\}, la ...

only adjectives that occurred 1K times in the training corpus, as this is the most common kind of adjective in Italian.
We excluded all cases in which the adverb-adjective combination occurred in the training corpus, in either feminine or masculine form.

\paragraph{Article-Adjective Number Agreement}
(3) adv-aeadj: la ADV \{ADJa, ADJe\}, le ADV \{ADJe, ADJa\}

Similar here, but we took a 500-occurrences threshold, as feminine plurals are less common.
Further, we manually removed adjectives that did not combine well semantically with the adverbs under consideration (pi{\`u}, meno, tanto).


Results are shown in Table~\ref{tab:ital-agr-results}.
The word LSTM shows the highest overall performance, closely followed by the LSTM CNLM.
The RNN performs well on adjective gender, and considerably worse than the CNLM on the other tasks.
For the CNLMs, the most challenging task was article-noun gender agreement.



\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l|l|l|l}
      \multicolumn{1}{c}{}&\emph{LSTM CNLM}&\emph{RNN CNLM}&\emph{Word LSTM}\\
% eadj-aonoun
	    Noun Gender & 97/90  & 84/73 & 99/96 \\
%      adv-aoadj
	    Adj. Gender & 99/100 & 100/97 & 98/100 \\
% adv-aeadj
	    Adj. Number & 99/99 & 99/70 & 100/100 \\
    \end{tabular}
  \end{center}
  \caption{\label{tab:ital-agr-results} Results for morphosyntactic tests in Italian}
\end{table}





%Discussion case-by-case, including how we control for n-gram frequency
%and length.
%
%Results table with a row for each pattern and a column for each model.


\subsection{Lexical semantics}
\label{sec:semantics}

In English, because that's where we have resources available.

%Correlation with one or more word similarity sets.

%Comparison to word-based NLM (rather than word2vec or such, which is
%specifically tuned for semantics).

We turn to the Microsoft Research Sentence Completion task \cite{Zweig:Burges:2011}.
choose between words mostly of the same syntactic category, requires world knowledge for humans to solve, thus complements the morphosyntactic tests in the previous section

The domain of the task (Sherlock Holmes novels) is very different from the Wikipedia dataset we are using; thus we addionally trained our models on the training set provided for the task, which sonsists of 19th century English novels with the same hyperparameter settings.
We both consider a fresh model trained on that data, and initializing it with the Wikipedia model.
For comparison, we report results (KN5 from \cite{Mikolov:2012}, LSTM from \cite{zhang2016top}) from previous work that were trained on the 19th century novels dataset (but the LSTM from that work had Glove embeddings). % \cite{zhang2016top} has a nice table if we want to report more

The CNLM outperforms KN5 in the setting where both are estimated using only the novels dataset.
It also outperforms a previously reported word LSTM (that had Glove embeddings), and -- when including the Wikipedia data -- approaches the SOA, held by approaches developed for the completion task \cite{woods2016exploiting}.

The vanilla RNN is not successful, contrasting with \emph{word}-based vanilla RNNs, whose performance, while below that of LSTMs, is much stronger.

\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l|l|l|l}
      \multicolumn{1}{c}{}& Model \\
% eadj-aonoun
LSTM CNLM	    &      34.1/59.0/59.2 \\
	    RNN CNLM &     24.3/24.0/27.1 \\
	    Word LSTM \\ \hline
	    Random & 20 \\
	    KN5   & 40.0 \\
            Word RNN & 45.0 \\
	    Word LSTM & 55.96 \\
Skipgram + RNNs \cite{Mikolov:etal:2013b} & 58.9 \\
LdTreeLSTM \cite{zhang2016top} & 60.67 \\
            \cite{woods2016exploiting} &  61.44 \\
    \end{tabular}
  \end{center}
  \caption{\label{tab:msr-completion-results} Results on MSR Sentence Completion}
\end{table}




