\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}

\usepackage{linguex}

\title{Response to Editor and Reviewers}
\date{February 2019}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{url}

\begin{document}

\maketitle


\section{Introduction (general response)}

We thank the editor and the reviewers for their insightful comments, which greatly helped us to strengthen the paper. We believe that we present now a stronger, more coherent story than in the first version.

The editor letter and the reviews pointed out that the paper contained too many experiments presented too succinctly, and that the general purpose of the study was not clear. Several specific issues were also raised, in particular concerning the phonology and segmentation sections.

The revised version focuses on the question of whether models that have no hard-coded word lexicon and are not exposed to explicit wordhood cues during training can handle linguistic phenomena, such as the ones we analyze in the morphology, syntax and semantics sections, that are traditionally seen in linguistics as being word-mediated. We relate this analysis to the current discussion in theoretical linguistics on the primacy of the notion of word. We have consequently rewritten large parts of the introduction and of the conclusion, to strengthen the theoretical message. We have removed the phonological experiments, since they do not address the issue of word-hood. The segmentation section has been completely re-designed and rewritten. It is now the last experimental section of the paper, where it fits the general story more naturally, by starting to address the question of \emph{how} a word-less model can succeed (at least to some degree) in word-mediated tasks such as the ones explored in the previous sections.

Following the lead of one reviewer, we searched more thoroughly for ``boundary units'' that the network might specialize to track word/morpheme boundaries, along the lines of Kementchedjhieva and Lopez. We were able to find such units, and the segmentation section is now a detailed analysis of their behaviour. This greatly simplifies the methodology, and, we believe, establishes a stronger connections with the general purpose of the paper.

By removing the phonological experiments and changing the methodology of the segmentation section, some of the claims and conclusions that the reviewers found most problematic are now gone. Moreover, thanks to the further space afforded by this and by the extra pages granted by the editor, we now explain our experiments in more detail. In particular, each experimental section starts now with a thorough discussion of the goal of the experiments and of the methodology we adopt.

We have rewritten the related-work section, being careful to avoid the ``defensive'' tone that had been noticed by a reviewer. We have further added a new sub-section on the theoretical debate on words in linguistics, as well as a few references that escaped us the first time around.

Finally, we have checked the whole paper for unwarranted claims, and we discuss the mediocre performance of our model in number prediction more thoroughly.

We describe other changes we made below, where we respond in detail to the comments by the editor and the reviewers. Note that we have tried to make each response standalone, modulo this general introduction. This came at the cost of some redundancy across responses.


\section{Response to the Editor}

\textbf{The main objections are that the many small insights in the individual experiments do not add up to a concrete claim about what these models learn, and they definitely are not strong enough on their own to hold up the broad claims that frame the paper, which encompass language acquisition, multilinguality, phonology, morphology, syntax, and semantics! See especially reviewer C's comments, which suggest that the paper may actually be clearer with less material, more precisely described; reviewer A's comments, which suggest that the paper should tone down its claims and make them more concrete; and the paper itself, which acknowledges that ``our results are preliminary in many ways'' (line 967).}

\textbf{In this case, my prescription is simply to present a concrete claim that is carefully supported by a coherent set of experiments. But this prescription is in fact vague: many different subsets of these results could be framed in different ways, possibly requiring different additional sets of supporting experiments. I don't feel it's my place to make that choice for you, so I've given you a (c). But for what it's worth, I suspect that making this paper TACL-worthy would require an amount of work on the short side of the 3-6 month period suggested for a (c) review.}

The paper has now been re-purposed to focus on the study of how word-less models can handle phenomena that are traditionally seen as word-mediated. We removed the phonological experiments, that become irrelevant in this perspective, completely re-hauled the segmentation section with new experiments that are both simpler and more intuitive than the original ones, and rewrote the introduction, related work and conclusion to frame our claims more clearly. Thanks to the extra space, we could also discuss all the other experiments in more detail.


\section{Response to Reviewer A}

\textbf{I think the study is worthwhile, but I think the authors need to be far more cautious in the claims they are making about what these models learn. It would be more beneficial to reflect on how these tasks \emph{begin} to inform us about what kinds of linguistic structure language-model trained neural nets can ``learn''.}

We have re-focused the paper on the question of whether word-less models can handle some linguistic phenomena that are intuitively word-mediated, and checked the whole paper for unwarranted claims.
\newline

\textbf{(1) The choice of languages should be motivated up front. Why English, German and Italian, which are all closely related? Why only three?}

We now motivate this in the introduction: ``\textit{We evaluate our character-level networks on a bank of linguistic tests in German, Italian and English. We focus on these languages due to resource availability and ease of benchmark construction. We also think that these well-studied synthetic languages with a clear, orthographically-driven notion of word are a better starting point to test non-word-centric models, compared to agglutinative or polysynthetic languages, where the very notion of what counts as a word is problematic.}''

Note that, as we discuss in the introduction to the section on syntactic tests (Section 4.2), the nature of character-level models makes it problematic to use existing benchmarks. Thus, any further language to be added requires considerable extra manual effort.
\newline

\textbf{(2) The very first evaluation (``Discovering phonological classes'') is oddly imprecise and impressionistic. Why should the reader take the authors' word for it that ``it definitely suggests that the CNLM has discovered a fair deal about the features organizing the phonological system of the language.'' This should be replaced with something quantitative or at least more objective, or dropped.}

We dropped the section about phonology.
\newline

\textbf{(3) The authors claim to be testing whether the CNLM develops an implicit notion of words, but the testing methodology involves a supervised training step. The paper needs to be much clearer about how this is actually testing whether the unsupervised system has an implicit notion of ``word''. (Similar remarks hold for the morphology tests.)}

In the new segmentation experiment (Section 4.4), we are focusing on the behaviour of single boundary units. We use training data simply to find a threshold on the value of these units that maximally separates boundary positions from other positions. If the boundary unit did not have a tendency to pick up boundaries, there is no way in which setting a single scalar threshold on its values would lead to anything better than random classification accuracy.

Concerning the morphological experiments, we have now tried to clarify the method of ``diagnostic classifiers'' that we adopted from the recent literature, where it is becoming a standard for analyzing neural models of language (see, e.g., among the references cited in our article, Shi et al., 2016, Adi et al., 2017, Conneau et al., 2018, Hupkes et al., 2018). The main idea of the method is to use activations produced by a pre-trained, fixed model as input features to a classifier of the target distinction. As the classifier can only rely on the features provided by the model, if it is able to successfully distinguish the property of interest, it means that the latter is already encoded in the activations provided by the model. Since we use a shallow linear classifier and extremely small training sets (20 examples in total for part of speech and 30 examples in total for number), we do not see how the classifier could induce the target distinctions, if they were not already saliently encoded in the representations provided by the pre-trained model. This is how we explain the approach in the paper (Section 4.1):

``\textit{We use here the popular method of ``diagnostic classifiers'' (Hupkes et al. 2017). That is, we treat the hidden activations produced by a CNLM whose weights were fixed after language model training as input features for a shallow (logistic) classifier of the property of interest (e.g., plural vs. singular). If the classifier is successful, this means that the representations provided by the model are encoding the relevant information.  The classifier is deliberately shallow and trained on a small set of examples, as we want to test whether the properties of interests are robustly encoded in the representations produced by the CNLMs, and amenable to a simple linear readout (Fusi et al., 2016). In our case, we want to probe word-level properties in models trained at the character level. To do this, we let the model read each target word character-by-character, and we treat the state of its hidden layer after processing the last character in the word as the model's implicit representation of the word, on which we train the diagnostic classifier.}''
\newline

\textbf{(4) The results of the pluralization study seem quite equivocal. In particular, the fact that the Umlaut plurals aren't properly modeled suggests that it's \emph{not} picking up on an abstract notion of ``plural''. The paper doesn't seem to acknowledge this sufficiently, either here or especially in the conclusion.}

Thanks for raising this issue. We ran a detailed error analysis of this experiment, and we found a confound in our data: nearly all training plurals contained \textit{-e-} as final vowel. When we removed the confound, we obtained the overall more interpretable results we report in Table 3 of the revised version. Now, on the one hand, the LSTM-based character-level model is clearly above chance even on Umlaut generalization. On the other hand, its performance if far from great in this case. We thus present a more cautious conclusion--there is definitely \emph{some} degree of generalization (new forms in \textit{-r}, and above-chance performance for the Umlaut class), but it is not a completely abstract one, as it applies more reliably when generalizing to new suffixation patterns, than to a fully non-concatenative process. We discuss this in the section on number (final part of Section 4.1), but we also get back to it in the conclusion (Section 5), to hedge our claims. We have also made a general pass through the abstract and the introduction, to tone down any claim that sounded too optimistic given the results.
\newline

\textbf{(5) That ``case subcategorization'' is represented by testing exactly one preposition in one language seems very narrow. Also, unlike German verbs which can be separated from their objects, P-NP sequences are not likely to be broken up, so this seems like something pretty surfacy/sequential and not really convincingly ``syntax''.}

We agree that this is a limited test, and we made sure that, in the new version, the preliminary nature of our results is emphasized (we also renamed the section ``Prepositional case subcategorization'', to stress the limited scope). The need to control for length of stimuli in characters makes the construction of these benchmarks very difficult. Ad the same time, we sampled the nominalized adjectives from a pool of thousands, and used more than 1.5K different sentential contexts. Thus, while it is true that we are only studying one specific case assignment phenomenon (case assignment by the preposition \textit{mit}), we are confident that our results for this specific phenomenon are fully general.

We did not understand your second point. In a phrase such as \textit{mit der sehr roten}, the form of the final (nominalized) adjective \textit{roten} is determined by the fact that the latter is embedded in a noun phrase governed by the non-adjacent preposition \textit{mit}. Moreover, only the full sentence context licenses the syntactic structure in which \textit{mit} is the head of the phrase governing the NP. Finally, as the n-gram and preposition-less controls show, surface statistics cannot account for model behaviour, so that some degree of structure-sensitivity is called for. Consequently, while we fully agree that this is a rather simple form of syntax, we respectfully disagree with your claim that this is not convincingly syntax.

We took various steps to clarify and position the experiment. First, we added a discussion of the general approach and important methodological issues at the beginning of the section on syntactic dependencies (Section 4.2). Second, the subcategorization experiment part has been re-written and extended, to explain in more detail and with examples what we test here. As above, in the paper as a whole we make sure we are in any case not making any overly general claim about ``syntax''.
\newline

\textbf{(6) The conclusion seems to over-claim compared to what the paper is actually showing. Most egregiously, I don't think that the sentence completion task establishes knowledge of ``basic semantics''. The syntactic agreement phenomena results are also somewhat equivocal (see detailed comments below) and the word units results rely on a supervised training step.}

We edited the conclusion (and introduction/abstract) to make our claims more precise and less over-reaching. See our reply above concerning the point on word segmentation, and replies above and below on the syntactic and semantic tests.
\newline

\textbf{Sec 2: How does this related work inform the questions you are asking? (The literature review reads as 'defensive', i.e. trying to prove that the work in the paper is novel, rather than situating the work with respect to existing literature.)}

We rewrote the related work section (Section 2). Given the new focus we emphasize in the introduction, we added a subsection on the theoretical primacy of the word. We have moreover edited the other subsections, removing any 'defensive' claim, and emphasizing instead what previous work has already contributed to our goals.
\newline

\textbf{Sec 2: This paper may also be relevant: Ettinger et al 2018 `Assessing Composition in Sentence Vector Representations'
\url{https://urldefense.proofpoint.com/v2/url?u=https-3A__aclanthology.coli.uni-2Dsaarland.de_papers_C18-2D1152_c18-2D1152&d=DwIBaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=eW1NdU8kpHF5nrq2Z__Rnw&m=8858iCmrOZOuNi3cLSQjugg77jFv3IG_TiJzC3Wh2ZU&s=C8soDo9_d3O47KTapHf4ZBX9C7Ryoz8J8nRZ8n7Mjqw&e=}}

Thanks, we added the reference.
\newline

\textbf{ln 209 It's not clear to me what ``in a localist fashion'' means.}

We rephrased, removing this unclear expression.
\newline

\textbf{ln 240 Does ``We used LSTM cells for WordNLMs'' mean something different from ``We only tested a word-level LSTM and not a word-level RNN''? If so, what?  Also, why not do the word-level RNN?}

We rephrased to confirm that this is indeed what we meant. We expect the WordNLM to act as an upper bound for our character-level models, in the word-mediated tasks we consider. For this reason, and because the paper does not focus on the analysis of word-level models per se, we thought it would make sense to directly adopt an LSTM model, as there is a vast literature indicating superiority of (word-based) LSTMs to vanilla RNNs, both in terms of NLP task performance and in capturing linguistic phenomena. We included instead a character-level RNN because we are concentrating on character-level models, and thus we want to ascertain to what extent, in this domain, LSTM cells are crucial to good model behaviour, if at all. Given that the paper is already reporting many analyses, and word-level models are only introduced for comparison purposes, we fear that adding results for a word-level RNN would affect the clarity of the paper, without bringing new insights on our specific problem.
\newline

\textbf{ln 325 ``The LSTM assigns higher probability to the acceptable bi-grams in all but two cases.'' Are the ratios of ``~1'' being counted as ``higher''? Why?  Similarly the caption to Table 2 says ``Values $> 1$ in bold'', but ``~1'' is in bold (in two places).}

We rounded these values, so that ``~1'' really means <1.01. This was an unfortunate choice. We have however now entirely removed this section from the paper.
\newline

\textbf{ln 385 What would be the linguistic basis for wider contexts helping with phoneme classes? (Long-distance phonological phenomena are relatively rare, and none---things like vowel harmony--immediately come to mind for the languages tested.)}

We agree, and the phonological results would definitely deserve further study. However, as we have now removed this section from the paper, we will leave such study to future work.
\newline

\textbf{ln 417 Why 20 characters? Isn't that way longer than most words, even in German?}

This was an arbitrary threshold, and we found out empirically that changing it did not have much impact on performance. However, this experiment has now been removed, replaced by a simpler one that does not require picking such arbitrary hyperparameters.
\newline

\textbf{ln 475 If you're working from phonological properties, why would fixed expressions turn up? Is there any reason to believe that in their orthographic form the internal word boundaries of fixed expressions are less like other word boundaries?}

While we have now removed this specific analysis, we believe that, for a system trained on the language modeling objective, it makes sense to chunk fixed expressions together. It is precious distributional information that the sequence \textit{Hong} is almost certainly followed by \textit{Kong} in an English corpus.
\newline

\textbf{ln 516 What was the training set used for the Berkeley Parser to be able to parse German?}

We have now removed this experiment.
\newline

\textbf{ln 546 ``unambiguously tagged in the corpus'':  I think it would be useful to remind the reader here that these aren't gold tags but come from TreeTagger (right?)}

We now explicitly state that these are TreeTagger annotations.
\newline

\textbf{Table 5 I don't understand what the last two lines are. Is WordNLM$_{subs}$. without OOV and WordNLM the full test set? If so, then ln 578 ``the word-based model fares better'' doesn't seem to make any sense---WordNLM scores \emph{lowest}.}

You are indeed right here. However, we think that, as we want to compare the performance of character-level models with what the WordNLM could do in principle, when factoring out the OOV problem, we think that both scores are interesting. We have now tried to clarify what these scores are by adding the following passage to the subsection on Word classes of Section 4.1:

``\textit{Unlike CNLMs, the WordNLM cannot make educated guesses about words that are not in its training vocabulary. These OOV words are by construction less frequent, and thus likely to be in general more difficult. To get a sense of both ``best-case-scenario'' and more realistic WordNLM performance, we report its accuracy both excluding and including OOV items (WordNLM$_{subs}$ and WordNLM in Table 2, respectively).}''

We have moreover added a warning to all table captions that the subs results are not comparable to the other results.
\newline

\textbf{ln 582 ``We study German as it possesses nominal classes that form plural through different morphological processes'' This is also true in Italian!}

While it is true that Italian also posseses different means to form plural, its system is more limited, more regular, and based on more systematic phonological and gender-based cues. Moreover, as masculines in \textit{-o} and \textit{-a} and all nouns in \textit{-e} form plurals in \textit{-i}  (\textit{gatto/gatti}, \textit{poeta/poeti}, \textit{badante/badanti}), this leaves pretty much only the feminine \textit{-a/-e} class for testing. This might still be interesting to explore, but given the number of experiments already present in the paper, we believe it is reasonable to leave it to future work.
\newline

\textbf{ln 589 Both of the cites given for ``German UD treebank'' seem to be about the UD project in general. Surely there's a specific citation for the German UD treebank that should be included to give those researchers credit for their work.}

Good point, we fixed this.
\newline

\textbf{ln 661 ``To avoid phrase segmentation ambiguities, we present phrases surrounded by full stops.'' I'm not sure what this means. What is the system presented with at test time? Just a phrase like in (1) (with only one article)? Why would not having full stops (before and after??) lead to ambiguity?}

Yes, the system was presented with phrases such as (1) surrounded by full stops. As the character model is not trained with whitespace, we use full stops to signal the beginning and ending of the phrase. Consider the \textit{der/die/das sehr rote Baum} example. At the right edge, if we do not append the period, the last character parsed by the model would be the \textit{m} of \textit{Baum}. At this stage, the model would have no reason to prefer the masculine (\textit{der}) to other articles, as what follows could be either a delimiter (such as the full stop), indicating that the noun is indeed the masculine from \textit{Baum}, or another character that might still be part of the same word. For example, the string could continue with \textit{wolle}. \textit{Baumwolle} is a feminine noun, requiring the article \textit{die}. Although some nouns will be more likely than others, we cannot fairly evaluate the model until we have presented it with an item disambiguating between different noun prefixes. Similarly, on the left, we let the model start by reading a full stop as a cue that we want it to treat the \textit{der/die/das} sequences as word initial. If we initialized the mode with other states (e.g., random ones), we could not be sure that the model would not then treat these sequences as suffixes, as it would have no specific reason to assume they are word-initial.

We tried to clarify these points in the new writeup. In the introduction of the section about syntactic dependencies (Section 4.2), we added the following passage:

``\textit{Consider the German phrase in (1) below. For a word model, two items separate the article from the noun. For a (space-less) character model, 8 characters intervene until the noun onset, but the span to consider will typically be even longer. For example, \emph{Baum} could be the beginning of the feminine noun \emph{Baumwolle} ``cotton'', which would change the agreement requirements on the article. So, until the model finds evidence that it fully parsed the head noun, it cannot reliably check agreement. This will typically require parsing at least the full noun and the first character following it.}''

Then, when introducing the relevant task, we state:

``\textit{To avoid phrase segmentation ambiguities, we present phrases surrounded by full stops (see the \emph{Baum}/\emph{Baumwolle} example above).}''
\newline

\textbf{ln 744 ``as these often reflect lemmatiziation problems'': Are these problems with TreeTagger, your system, or something else?}

These are problems with the TreeTagger. We now clarify in a footnote that the TreeTagger occasionally failed to remove the inflectional suffix \textit{-r} when lemmatizing.
\newline

\textbf{ln 750 When would German ever have discontinuous NPs?}

As we now explain in footnote 11, this can happen because of extraposition, that leaves part of the noun phrase separated from the rest by the verb. Here is an example for the treebank:

\exg. ich sah   mich   plötzlich mit  [einer Freiheit] konfrontiert , [die   ich früher  gerne  gehabt hätte] \\
I   found myself suddenly  with [a     freedom]  confronted   ,  which I   earlier gladly had    had \\
\textit{`I suddenly found myself confronted by [a (kind of) freedom], [which I would have liked to have previously]'}

where the relative clause is separated from the remainder of the NP (the constituent \textit{[einer Freiheit]}) by the verb \textit{konfrontiert}.
\newline

\textbf{ln 752 Is it well established that RNNs \& LSTMs have the same probabilistic bias for shorter sequences that e.g. HMMs do?}

We refer here to a problem with the methodology of comparing phrase likelihoods when phrases have different lengths. Phrase likelihood is given by the product of single-character probabilities, that range between 0 and 1. Thus, \emph{all else being equal}, a longer sequence can only be as likely or less likely than a shorter one. We tried to clarify the issue by adding the following remark in the introduction to the syntactic agreement section (Section 4.2):

``\textit{[S]ince a character-level language model assigns a probability to each character of a phrase, and the phrase likelihood is the product of these values (all between 0 and 1), minimal sets must be controlled for character length.}''

Then, when discussing this experiment, we state:

``\textit{Note that here the correct form is longer than the wrong one. As the overall likelihood is the product of character probabilities ranging between 0 and 1, if this introduces a length bias, it will work against the character models.}''
\newline

\textbf{ln 774-776 I found this too terse. What is the n-gram count model? Why omit the sentence environment?}

We have largely rewritten the subsection describing this experiment, trying to add a better motivation for the sentence environment method and including examples. The n-gram probabilities from the beginning of the sentences are always 0, and they would not, in any case, provide more information about case than the phrase context does. So, by omitting the sentence environment, we are obtaining a stronger n-gram baseline.
\newline

\textbf{ln 778 What stimuli not including the preposition? Where are these described?}

These are the control stimuli that we created by feeding the model with a version of each original stimulus with the prefix up to and including the preposition stripped off. For these control stimuli, the model could only rely on the independent probabilities of the tested adjective forms. We have now rewritten the previous paragraphs, and added examples to clarify what we mean.
\newline

\textbf{4.4.2 If the words occur in the corpus, they presumably occur with their article, so it's not immediately clear to me that the stimuli don't occur in the corpus. Perhaps the unattested n-grams are the adj+N combination?}

Indeed, we removed the confusing paragraph you refer to, and we are now more explicit about which combinations do not occur in the corpus, making the n-gram baseline perform at chance level:
\begin{itemize}
\item Article-noun gender agreement: ``\textit{adjective-noun combinations that occurred in the training corpus were excluded, so that an n-gram baseline would perform at chance level.}''
\item Article-adjective gender agreement: ``\textit{We excluded all cases in which the adverb-adjective combination occurred in the training corpus}''
\item Article-adjective number agreement: ``\textit{Again, no adverb-adjective combination was attested.}''\newline
\end{itemize}
%\newline

\textbf{ln 835 What does ``strong semantic anomaly'' mean and how is it checked for?}

We clarified this: ``Here and below, stimuli were manually checked removing nonsensical adjective-noun (below, adverb-adjective) combinations.''
\newline

\textbf{ln 890 Threshold for what? (I couldn't quickly figure out what the 500 occurrence were \emph{of}, nor what to compare to ``above'').}

We clarified: ``Stimulus selection was as in the last experiment, but we used a 500-occurrences threshold for adjectives, as feminine plurals are less common''
\newline

\textbf{ln 919ff I'm extremely skeptical of the claims about the sentence completion task. In particular, no language model has information about ``syntax, lexical semantics, world knowledge, and pragmatics'' beyond what can be characterized in purely distributional terms --- i.e. what words share what kind of distributional similarity with what other words. That will be a partial reflection of part of speech (syntax-ish) and lexical semantics, but it is no way ``world knowledge''. Furthermore, models don't ``realize'' anything let alone ``that [friend and mistress] are human beings''.}

We are now much more careful about our claims, explicitly referring to ``the shallow form of semantics required in a fill-the-gap test'' when we introduce this task, and with hedges elsewhere. We removed the broad claim you rightly criticize, and we explain the nature of the test more clearly in the relevant section (Section 4.3):

``\textit{The creators of the benchmark took multiple provisions to insure that success on the task implies some command of semantics. The multiple choices were controlled for frequency, and the annotators were encouraged to choose confounders whose elimination required ``semantic knowledge and logical inference'' (Zweig and Burges 2011).  For example, the right choice in ``Was she his [client|musings|discomfiture|choice|opportunity], his friend, or his mistress?'' depends on the cue that the missing word is coordinated with ``friend'' and ``mistress'', and that the latter are animate entities.}''

We also renamed Section 4.3 ``Semantics-driven sentence completion'' (from the original ``Semantics'') to more clearly delimit its scope.
\newline

\textbf{ln 965 ``somewhat deeper linguistic templates'' seems like an overclaim.}

We rewrote the conclusion to avoid overclaims. In particular, we reformulated this statement as follows: ``The character-level models consistently outperformed n-gram controls, which shows that they tap into more abstract patterns than local co-occurrence statistics.''
\newline

\textbf{ln 990 Why didn't you include polysynthetic and agglutinative languages in your testing? There are pretty good resources available for Inuktitut and Turkish, respectively, for example.}

We are currently working with a typologically varied sample of languages, however this is extremely laborious and requires a good command of the relevant languages, since we need to develop, in each language, benchmarks that are appropriate for character-level models. Moreover, we now made it clear that the central pursuit of the paper is to ascertain to what extent word-less models can handle word-mediated words. In this perspective, we think there are good reasons to start with languages with a clearly defined conventional notion of word, instead of with languages where the very idea of what is exactly a word is problematic.

We motivate our language choice in the introduction as follows:

``\textit{We evaluate our character-level networks on a bank of linguistic tests in German, Italian and English. We focus on these languages due to resource availability and ease of benchmark construction. We also think that these well-studied synthetic languages with a clear, orthographically-driven notion of word are a better starting point to test non-word-centric models, compared to agglutinative or polysynthetic languages, where the very notion of what counts as a word is problematic.}''
\newline

\textbf{ln 991 ``the common view that'': This should come with citations. Places to look are work on Construction Grammar (authors such as Chuck Fillmore and Paul Kay) and also work by Ray Jackendoff.}

Thanks, we added references.
\newline

\textbf{ln 13-14 recently reached $\Rightarrow$ has recently reached}

Thanks, this sentence has now been removed.
\newline

\textbf{ln 096 as it goes $\Rightarrow$ as it gets}

Fixed.
\newline

\textbf{ln 149 model $\Rightarrow$ models?}

Thanks, fixed.
\newline

\textbf{ln 431 ad hoc doesn't need a hyphen}

This part is now gone.
\newline

\textbf{ln 531 can discover about $\Rightarrow$ can discover -or- can discover information
about}

This part is now gone.
\newline

\textbf{ln 622 I'm not sure what ``the latter'' is supposed to refer back to.}

Thanks, we rephrased (it referred to word embeddings).
\newline

\textbf{ln 720 the Universal Dependencies $\Rightarrow$ the German UD treebank}

Thanks, fixed.

ln 996 capable to flexibly store $\Rightarrow$ capable of flexibly storing

Thanks, this is no longer in the revision.


\section{Response to reviewer B}

\textbf{1. Section 4.2 presents results on word segmentation. The paragraph starting on line 464 qualitatively investigates the errors made by the CNLM trained on Wikipedia test (note: it would be beneficial to state Wikipedia right at the beginning of the paragraph, rather than at its end). It would though be more interesting if this were a comparison between the Bayesian and the CNLM model, rather than just analyzing the CNLM.  Because, albeit the fact that ``CNLM performance is comparable'' (ref. to Table 4), a close look reveals that there is quiet a gap of the two models in terms of precision on inducing lexical word types. A comparative analysis would shed some light here, it might be that the LSTM gets frequent types right but misses other types, compared to the Bayesian method constructed with a lexical bias in mind.}

Thanks, this is a very good point. However, as we have completely rewritten this section (current Section 4.4), removing supervised classification based on model-produced probabilities altogether, we have also removed the comparison with the Bayesian model, that we felt was problematic in any case as we were comparing very different algorithms with very different purposes.
\newline

\textbf{2. For the first analysis (phonological classes induced by the output embeddings) results for German only are provided in Figure 1. The paper should include plots for all three languages, as there is no clear motivation why one was selected. There should be space to include all three plots.}

As we changed the story in the paper (see the general introduction to the responses above), the part about phonology has been removed.
\newline

\textbf{3. What really surprised me is the bad performance of the vanilla RNN compared to the LSTM on the bigram acceptability judgment task (lines 382-383). This is in fact dramatic, as the model only needs to consider adjacent characters. At first it seems the model is underfit, but then the RNN performs reasonably well on other tasks, sometimes even being close to the LSTM (e.g., adj-gender agreement on Italian, Table 7) and perplexity scores are reasonable as well. Maybe a further discussion in light of training data properties and locality of the task might shed some light here (how long are the paragraphs the models are trained on?). Finally, what is also surprising is that the RNN does not improve with in-domain training data for the last task (sentence completion, see line 2 in Table 8). Why is the vanilla RNN not improving? Would it help to fine-tune on the in-domain data?}

We agree that the RNN results are puzzling, especially in the character bigram acceptability task. Since the part on phonology has been removed from this paper, we will look at that question in more detail in other work. Concerning the failure of in-domain data to improve RNN performance in the sentence completion task, we now discuss this in the relevant section (Section 4.3), although we do not really have a full grasp on the causes. We agree with you that one possibility here is that the RNN would need more training data, and this could be indeed ascertained by training out of domain and tuning in domain. However, we feel that, for space and flow reasons, this is better left to future work.

The new analysis of the boundary units suggests that the RNNs are quite a bit worse than the LSTMs at tracking word boundaries, which might be the reason why the RNNs generally lag behind in word-mediated tasks. As you observe, this does not explain the cases when they do perform well, but, when the results are considered as a whole, it is pretty clear that their performance is, at least, very unstable.

Concerning the possibility of underfitting, we have added a caveat in footnote 6 (commenting on the fact that we stopped training after 72 hours):  ``This was due to resource availability. The reasonable language-modeling results in Table1 suggest that no model is seriously underfit, but the weaker overall RNN results in particular should be interpreted in the light of the following qualification:  models are compared \emph{given equal amount of training, but possibly at different convergence stages}.''
\newline

\textbf{4. The paper does a great job in discussing related work. I though kept wondering about the difference with Kementchedjhieva \& Lopez (2018). While overall results are in line (RNN-LMs do capture morphological properties), the paper is very brief on reporting an interesting divergence: ``we could not replicate the result with our model'' (on a single neuron tracking morpheme boundaries). It would be interesting to know if this is due to the different modeling setup (e.g., would this also hold for the model trained with white-space, footnote 6?) or what other reasons there could be at play.}

Thanks for encouraging us to look into this discrepancy more closely. We found that in our original analysis we overlooked the presence of boundary units in our models. Kementchedjhieva and Lopez found their unit by visual inspection of unit behaviour on a small sample. As we have more models to evaluate, and wanted stronger quantitative support for claiming that a unit is tracking boundaries, we looked instead at units that correlate with end-of-word positions. In the initial analysis, we required what was probably an unrealistic threshold for the resulting correlation score ($>.7$). By lowering that threshold, we instead found units that are still strongly correlated with word endings (as high as $.69$), and, more importantly, for whose further qualitative and quantitative analysis confirmed the role of boundary trackers. Besides aligning our findings with those of Kementchedjhieva and Lopez, we think that this allowed us to present a better segmentation analysis, and a more coherent story overall.
\newline

\textbf{- Table 3: check F1 score for Italian (should be 59 rather than 60)}

Thanks, the table has now been removed (this was a rounding issue).
\newline

\textbf{- Presentation of results in Table 3 and 4: use of different decimal places.}

Thanks, these tables have now been removed, and we checked that the current tables are coherent in terms of numerical format.
\newline

\textbf{- Colored figures are unreadable in b/w printing.}

We looked for color-blind-friendly color combinations. Unfortunately, we were not able to come up with a choice of colors that would be both acceptable for most types of color blindness and b/w-printing friendly.
\newline


\textbf{- line 936: in Figure 8 $\Rightarrow$ in Table 8}

Fixed, thank you.


\section{Response to Reviewer C}

\textbf{1. No usable datasets submitted.}

The TACL submission instructions prevent us from uploading supplementary materials, and thus no dataset can be submitted. We believe that the reviewer instructions for this field: ``If the authors state (in anonymous fashion) that datasets will be released, how valuable will they be to others?'', ask the reviewer to do some counterfactual reasoning: If the authors, who promised to release their data sets (see footnote 2), do indeed release the dataset, how valuable will this be? So, while you might find our benchmarks and resources more or less useful, we think that, according to the (somewhat contrived) TACL instructions, it is not accurate to state that we did not submit a usable dataset. We are happy to share the data with you through some channel suggested by the Editor.
\newline

\textbf{I think this is an interesting area of inquiry.  The experiments in this paper are extensive, but sometimes don't seem to fit the intent of the authors and/or are not clearly explained.  The abstract really focuses on the idea of removing spaces and still being able to recover words and morphology, but the experiments veer away from that pretty quickly (starting with experiment 5 below).}

We have now rewritten the paper to clearly re-focus it on the question that you highlight. However, rather than focusing solely on morphology, we justify a broader set of experiments as investigating to what extent our word-less models capture phenomena that are traditionally seen as word-mediated. The latter also include capturing relations between agreement features that are encapsulated in words and the simple form of lexical semantics probed by the sentence completion test. Please see the introduction to this response for a general outline of the main changes.
\newline

\textbf{In general, there are too many experiments crammed into this paper, and not enough explanation of the experimental set up, or careful consideration of results.  This paper is right at the page limit, so I think the authors should reconsider which experiments are most telling, and move some of the extraneous ones to supplementary material.}

Thanks for suggesting this. We have removed the phonological experiments, as they were after all not relevant for our purpose, and reduced the segmentation section (now Section 4.4) to a more focused study of boundary-tracking units in the models. Again, please see the introduction to this response above for an outline of the changes.
\newline

\textbf{I can't figure out from the TACL page if TACL allows supplementary material, but in any case, there's too much in these 10 pages to cover in the detail required for a reader to understand and be able to reproduce any of these results.}

TACL does not allow supplementary materials. However, by removing experiments and using the extra 2 pages granted by the editor, we hope we made the current materials more understandable and reproducible.
\newline

\textbf{1. Remove spaces, how does that affect perplexity/bits-per-char? I'm not convinced that removing spaces is a good proxy to the word segmentation problem infants and young children encounter, since they are exposed to much simpler language (single words, very simple sentences).}

Good points, thanks! We report bits-per-char when the models are trained with white space in footnote 8 of the revised version.

Concerning differences between what our model is fed and what children hear, we are now more explicit about it in the paper. Specifically, in the conclusion we state that: ``In terms of  comparison with human learning, the Wikipedia text our CNLMs receive as input is far from what children acquiring a language would hear. Future work should explore character/phoneme-level learning from corpora of child-directed speech. Still, arguably, by feeding our networks long-winded grown-up prose, we are making the job of identifying basic constituents harder than it might be when processing the short utterances characterizing early child-directed speech (Tomasello, 2003).''

We also address the question of single-word input in the new subsection of Related Work (first part of Section 2): ``Children are only rarely exposed to words in isolation during learning (Tomasello 2003),'' with a footnote specifying that: ``Single-word utterances are not uncommon in child-directed language, but they are still rather the exception than the rule, and many important words, such as determiners, never occur in isolation (Christiansen et al, 2005).''
\newline

\textbf{2. Cluster characters by their embeddings.  Do the cluster represent phonetics? This experiment is not repeated (or results are not shown) for the RNN.  No details are given for how the clustering was run (distance metric?) and the cutoff for clusters appears to be chosen arbitrarily.}

We agree that these experiments were too sketchy, and we removed them from the paper.
\newline

\textbf{3. Identify some acceptable and unacceptable bigrams in each language. Train on data with both sets removed, and then test if the held out bigrams
are assigned probabilities that are consistent with the acceptable/unacceptable categorization. Here, I am very surprised that the RNN did so terribly, to the point where I wonder if there is a bug in the analysis or code.  If there is no bug, I think a better explanation for this behavior needs to be brought forward. For example, perhaps the clustering as in Fig 1 would show that the phonological categories are not learned by the RNN, which would help to explain the lack of generalization we're seeing in this experiment (which requires learning phonological categories).}

We agree that these results are surprising. We did systematically searched for bugs in the experiment code, but we were not able to find any. We are reasonably confident about the RNN training procedure, given the reasonable BPC values they achieve, and their decent performance on other tasks. We will look into the issue in separate work, as this part has now been removed from the paper.
\newline

\textbf{4a. Word segmentation This experiment is not fully explained.  In particular the context PMI is unclear to me here, and needs more explanation.  But somehow they are creating features which they use to predict which characters start words}

We removed this experiment, and we report what we believe to be a much more straightforward one in the revised segmentation section (Section 4.4).
\newline

\textbf{4b. A small little experiment with a LDA word segmenting algorithm is included here, but so little detail is given that we can't draw much of a conclusion.  It's also trained on a different corpus, so it sticks out a bit.  Suggest this be put into a supplementary material section with more details.}

We removed this.
\newline

\textbf{4c. Error analysis'' This is actually fairly interesting and I appreciate this qualitative account}

Thanks! We replicated this error analysis, and added further qualitative insights, in the revised segmentation section (Section 4.4).
\newline

\textbf{4d. Compare PMI to hierarchical distance This experiment is really light on details and the accompanying figure 2 HAS NO LABELS WHATSOEVER.  No axis labels and no legend labels!  There is only one paragraph actually explaining this experiment, and it's not nearly enough to understand the results.}

We removed this experiment.
\newline

\textbf{At this point we begin to veer off course, and the models seem to be trained and/or tested on single words, which makes a bit of sense sometimes (e.g. experiment 5 below which uses the models trained in previous sections) but not always.}

We believe that your impression is due to lack of clarity on our behalf concerning how the experiments in the morphology section were run. We are not re-training the model. We are using the hidden activations provided by the pre-trained model after reading a word as features. These features are fed to a ``diagnostic classifier'' (see, e.g., among the references cited in our article, Shi et al., 2016, Adi et al., 2017, Conneau et al., 2018, Hupkes et al., 2018), that is trained to discriminate a particular property (e.g., number). The classifier is 1) shallow; 2) trained on a very small number of examples (not more than 30); 3) not given any other further information (e.g., about context) than what is pre-encoded in the model hidden activations. Thus, if the classifier is able to successfully distinguish the property of interest, it means that the latter must already be encoded in the activations provided by the model. This is how we now describe the approach in the paper (Section 4.1):

``\textit{We use here the popular method of ``diagnostic classifiers'' (Hupkes et al. 2017). That is, we treat the hidden activations produced by a CNLM whose weights were fixed after language model training as input features for a shallow (logistic) classifier of the property of interest (e.g., plural vs. singular). If the classifier is successful, this means that the representations provided by the model are encoding the relevant information.  The classifier is deliberately shallow and trained on a small set of examples, as we want to test whether the properties of interests are robustly encoded in the representations produced by the CNLMs, and amenable to a simple linear readout (Fusi et al., 2016). In our case, we want to probe word-level properties in models trained at the character level. To do this, we let the model read each target word character-by-character, and we treat the state of its hidden layer after processing the last character in the word as the model's implicit representation of the word, on which we train the diagnostic classifier.}''
\newline

\textbf{5. Nouns vs verbs: can they be classified using the final hidden state of a pre-trained model after reading the last char? I don't speak German, but this sentence doesn't make any sense to me ``requiring that they end in -en (German) or -re (Italian) (so that models can’t rely on the affix for classification), `` how would restricting the suffix (en, re) also restrict the affix?}

Thanks for catching the terminological mishap. We did indeed mean \emph{suffix}, and we have now clarified with examples (in the relevant part of Section 4.1): ``Verbal and nominal forms are often cued by suffixes. We removed this confound by selecting examples with the same ending across the two categories (``-en'' in German: ``Westen'' (``west''), ``stehen'' (``to stand''); and ``{-re'' in Italian: ``autore'' (``author''), ``dire'' (``to say'')).''
\newline

\textbf{The baseline here is an autoencoder LSTM trained on words in isolation.  This seems like a straw man, if only shown words in isolation this model is missing much of the context information that help the context-full LSTM tell verbs from nouns.}

We agree that the autoencoder is a straw man. While the autoencoder has no access to context, the character-level model does have access to information about word substrings, which is all the autoencoder can rely upon. We thus use the autoencoder as a control, to make sure the character-level models are making use of broader context, and not simply of possible word-internal substrings that correlate with part of speech. We tried to make this clearer (in the relevant paragraph of Section 4.1): ``While we controlled for suffixes as described above, it could still be the case that other substrings reliably cue verbs or nouns. We thus considered a baseline trained on word-internal information only, namely a character-level LSTM autoencoder trained to reconstruct words in isolation.  The hidden state of the autoencoder should capture discriminating orthographic features, but, by design, the latter has no access to broader contexts.'' 
\newline

\textbf{6. Can the model detect number: Here I'm unclear what this has to do with the model trained on space-free text.  The authors seem to be training on single words? ``For the training set, we randomly selected 15 singulars and plurals from each training class.}''

As we explained hopefully more clearly above and in the paper, this diagnostic classifier takes as input features the hidden activations produced by the (pre-trained, fixed) space-free models. We have in general rewritten the section to make the methodology clearer.
\newline

\textbf{The results show that the CNLM can't generalize to umlaut, but the explanation is lacking (suffix vs internal root vowel change).  Why? is the interesting question here.}

After removing a confound (nearly all diagnostic-classifier-training plurals contained ``-e-'' as final vowel), we obtained the overall more interpretable results we report in Table 3 of the revised version. Now, the LSTM-based character-level model is clearly above chance in Umlaut generalization. However, its performance is still weaker than for ``-r'' generalization. We conjecture that the model is learning, to some degree, an abstract notion of plural, but at the same time the presence of explicit suffixation reinforces its ability to retrieve such abstract number information. We discuss this in the section on number (final part of Section 4.1).
\newline

\textbf{There are many more experiments after this point, and the main themes of my critiques are the same.  There is not enough information given to fully understand these experiments (and thus replicating would be impossible). The figures have NO labels.  There is no careful consideration of results.}

We have revised all the sections, with headers explaining the general methodologies, and more details and examples for each experiment. We have also revised all figures and captions.
\newline

\textbf{line 242, the models were not trained until validation accuracy plateaued? That does not seem standard.  How can we know if these models are fit to compare against each other if we're not sure they're done training?}

This is an important point. We simply did not have enough resources available to train for longer than we did . We now added footnote 6 stressing this: ``This was due to resource availability. The reasonable language-modeling results in Table1 suggest that no model is seriously underfit, but the weaker overall RNN results in particular should be interpreted in the light of the following qualification:  models are compared \emph{given equal amount of training, but possibly at different convergence stages}.''
\newline

\textbf{The citations for the figures/tables are missing a lot of information.  It's nice to not have to scan through the text to figure out what each figure is showing, and many of the important details are left out of the figures+captions (e.g. the acceptable/unacceptable order in Table 2, what model is used to do the clustering in figure 1).  This is a little of personal preference (which is why I include it here in the minor comments), but it makes for an annoying first skim of the paper if you can't figure out any of the figures without.  E.g. the caption for Table 5: ``word class accuracy with standard errors. ...''  For what task???}

We revised all figures, tables and captions, and these issues should now be fixed.
\newline

\textbf{Tables with 9 cells, and 3 numbers per cell are pretty hard to parse e.g. Table 3/4.  A bar graph with just F1 would be nice, unless the authors actually think P/R make any contribution (they don't seem to talk about P/R in any detail).}

These tables have been removed.
\newline

\textbf{Table 3 just gives P/R/F1, I don't think the claim on line 428 (classify half the tokens correctly) can be inferred from P or R, rather, one needs accuracy.}

This analysis has now been removed.
\newline

\textbf{Section 4.2 needs some subheaders, there's too much going on here and it's hard to keep track of what the point of the current experiment is.}

We radically revised the segmentation section (now Section 4.4), so that it is hopefully more focused. However, subheaders are a good idea and we added them.
\newline

\textbf{A few tables/figures have STD or bootstrapped CI, but many do not. Would like to see them consistently throughout}

We present these variance estimates in the morphological experiments, where there is a randomized train/test selection for a diagnostic classifier that uses small training data sets. In the other experiments, there is no diagnostic classifier training or no randomized selection of a small training set. Standard errors/CIs would be very small (essentially imperceptible), due to the large number of stimuli. We hope that our methodological clarifications throughout the paper make our policy clear.
\newline

\textbf{Line 616: ``as above'' there's a lot of stuff above at this point, please refer to something more concrete}

We clarified the relevant procedure.
\newline

\textbf{Is table 8 really comparing results across corpora?  The top 3 models are trained on Wikipedia, and the bottom on Sherlock Holmes?  This is not a sound comparison, not sure what we're supposed to take away from this experiment}

The results on the left of the first 3 rows are obtained with a different training corpus (Wikipedia). All other results are with 19th century novels. The only function of the Wikipedia results is to show that in-domain training is crucial here. We revised the table formatting and the discussion in the text to clarify.


\bibliographystyle{plain}
\bibliography{references}
\end{document}

