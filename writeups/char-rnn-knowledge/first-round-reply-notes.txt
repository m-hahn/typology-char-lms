We thank the editor and the reviewers for their insightful comments, that helped us in re-focusing the paper, which we believe is now sending a stronger, more coherent message than in the first version.

The editor letter and the reviews pointed out that the paper contained too many experiments, that were not always clearly outlined, and did not make a clear case about their purpose. Several specific issues were also raised, in particular concerning the phonology and segmentation sections.

The revised version focuses on the question of whether out word-less models can learn to handle linguistic phenomena, such as the ones we analyze in the morphology, syntax and semantics sections, that are intuitively word-mediated. In this perspective, that we now thoroughly discuss in the introduction and in the conclusion, the phonological experiments do not fit the paper anymore, as they do not relate to the notion of word-hood. By removing them, we remove some of the claims the reviewers found most problematic, and we make room for a more thorough analysis of the other results. Moreover, under the new organization, the segmentation experiment has been moved to the last slot in the experimental section, as it now naturally addresses the question of whether the (relative) successes we observed in modeling word-mediated phenomena means that our models are implicitly keeping track of word boundaries.

We changed the main experiment in the segmentation section to test more directly this latter hypothesis. Instead of training a word segmenter on features derived from model-produced likelihoods, we directly probe the hidden state of the trained models for information about word breaks using a "diagnostic classifier".

We have also rewritten the related-work section, to remove the "defensive" tone that had been noted by a reviewer, as well as to place the paper in the broader contexts of studies on the primacy of the word in linguistics.

We describe other changes we made below, where we explain in detail how we addressed the comments by the editor and the reviewers.



E:

The main objections are that the many small insights in the individual
experiments do not add up to a concrete claim about what these models
learn, and they definitely are not strong enough on their own to hold
up the broad claims that frame the paper, which encompass language
acquisition, multilinguality, phonology, morphology, syntax, and
semantics! See especially reviewer C's comments, which suggest that
the paper may actually be clearer with less material, more precisely
described; reviewer A's comments, which suggest that the paper should
tone down its claims and make them more concrete; and the paper
itself, which acknowledges that "our results are preliminary in many
ways" (line 967).

=> revise claims, focus more on what character-based models learn
about word-centered phenomena

In this case, my prescription is simply to present a concrete claim that is
carefully supported by a coherent set of experiments. But this prescription
is in fact vague: many different subsets of these results could be framed in
different ways, possibly requiring different additional sets of supporting
experiments. I don't feel it's my place to make that choice for you, so I've
given you a (c). But for what it's worth, I suspect that making this paper
TACL-worthy would require an amount of work on the short side of the 3-6
month period suggested for a (c) review.

=> reread and revise claim (more precise claims about how
character-based model learns word-based phenomena, would naturally
lead to drop part about phonology, which could be left to a different
paper)

I am allowing you one to two additional pages in the revised version for
addressing the referees' concerns.

=> good

**************
**************

A (b)/(c):

Detailed Comments for the Authors

I think the study is worthwhile, but I think the authors need to be
far more cautious in the claims they are making about what these
models learn. It would be more beneficial to reflect on how these
tasks *begin* to inform us about what kinds of linguistic structure
language-model trained neural nets can "learn".

=> can be done by careful rephrasing

(1) The choice of languages should be motivated up front. Why English,
German and Italian, which are all closely related? Why only three?

=> can be done

(2) The very first evaluation ("Discovering phonological classes") is
oddly imprecise and impressionistic. Why should the reader take the
authors' word for it that "it definitely suggests that the CNLM has
discovered a fair deal about the features organizing the phonological
system of the language." This should be replaced with something
quantitative or at least more objective, or dropped.

=> drop phonology part

(3) The authors claim to be testing whether the CNLM develops an
implicit notion of words, but the testing methodology involves a
supervised training step. The paper needs to be much clearer about how
this is actually testing whether the unsupervised system has an
implicit notion of "word". (Similar remarks hold for the morphology
tests.)

=> use probing task (explaining at the beginning), and only show
syntax figure with PMI

=> explain probing methodology more clearly

(4) The results of the pluralization study seem quite equivocal. In
particular, the fact that the Umlaut plurals aren't properly modeled
suggests that it's *not* picking up on an abstract notion of
"plural". The paper doesn't seem to acknowledge this sufficiently,
either here or especially in the conclusion.

=> discuss more

(5) That "case subcategorization" is represented by testing exactly
one preposition in one language seems very narrow. Also, unlike German
verbs which can be separated from their objects, P-NP sequences are
not likely to be broken up, so this seems like something pretty
surfacy/sequential and not really convincingly "syntax".

=> discuss our nature of long-distance more clearly, add hedges

(6) The conclusion seems to over-claim compared to what the paper is
actually showing. Most egregiously, I don't think that the sentence
completion task establishes knowledge of "basic semantics". The
syntactic agreement phenomena results are also somewhat equivocal (see
detailed comments below) and the word units results rely on a
supervised training step.

=> clarify probing methodology

=> hedges about semantics

=> below on syntactic agreement

Sec 2: How does this related work inform the questions you are asking?
(The literature review reads as 'defensive', i.e. trying to prove that
the work in the paper is novel, rather than situating the work with
respect to existing literature.)

=> rephrase

Sec 2: This paper may also be relevant: Ettinger et al 2018 `Assessing
Composition in Sentence Vector Representations'
https://urldefense.proofpoint.com/v2/url?u=https-3A__aclanthology.coli.uni-2Dsaarland.de_papers_C18-2D1152_c18-2D1152&d=DwIBaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=eW1NdU8kpHF5nrq2Z__Rnw&m=8858iCmrOZOuNi3cLSQjugg77jFv3IG_TiJzC3Wh2ZU&s=C8soDo9_d3O47KTapHf4ZBX9C7Ryoz8J8nRZ8n7Mjqw&e=

=> checkout (done)

ln 209 It's not clear to me what "in a localist fashion" means.

=> OK

ln 240 Does "We used LSTM cells for WordNLMs" mean something different
from "We only tested a word-level LSTM and not a word-level RNN"? If
so, what?  Also, why not do the word-level RNN?

=> explain (still todo)

ln 325 "The LSTM assigns higher probability to the acceptable bi-grams
in all but two cases." Are the ratios of "~1" being counted as
"higher"? Why?  Similarly the caption to Table 2 says "Values > 1 in
bold", but "~1" is in bold (in two places).

=> this might go

ln 385 What would be the linguistic basis for wider contexts helping with
phoneme classes? (Long-distance phonological phenomena are relatively rare,
and none---things like vowel harmony--immediately come to mind for the
languages tested.)

=> this might go

ln 417 Why 20 characters? Isn't that way longer than most words, even in
German?

=> this might go

ln 475 If you're working from phonological properties, why would fixed
expressions turn up? Is there any reason to believe that in their
orthographic form the internal word boundaries of fixed expressions are less
like other word boundaries?

=> clarify (and this might go)

ln 516 What was the training set used for the Berkeley Parser to be able to
parse German?

=> clarify (done)

ln 546 "unambiguously tagged in the corpus":  I think it would be useful to
remind the reader here that these aren't gold tags but come from TreeTagger
(right?)

=> clarify (done)

Table 5 I don't understand what the last two lines are. Is WordNLM_subs.
without OOV and WordNLM the full test set? If so, then ln 578 "the
word-based model fares better" doesn't seem to make any sense---WordNLM
scores *lowest*.

=> report results with test sets redesigned to only include words in WordNLM vocabulary (and mention this in text)
(I think we decided to instead keep the previous setting, and explain it better)

ln 582 "We study German as it possesses nominal classes that form plural
through different morphological processes" This is also true in Italian!

=> explain

ln 589 Both of the cites given for "German UD treebank" seem to be about the
UD project in general. Surely there's a specific citation for the German UD
treebank that should be included to give those researchers credit for their
work.

=> do (done)

ln 661 "To avoid phrase segmentation ambiguities, we present phrases
surrounded by full stops." I'm not sure what this means. What is the system
presented with at test time? Just a phrase like in (1) (with only one
article)? Why would not having full stops (before and after??) lead to
ambiguity?

=> explain (TODO)

ln 744 "as these often reflect lemmatiziation problems": Are these problems
with TreeTagger, your system, or something else?

=> explain (TODO Michael)

ln 750 When would German ever have discontinuous NPs?

=> explain (TODO Michael)

ln 752 Is it well established that RNNs & LSTMs have the same probabilistic
bias for shorter sequences that e.g. HMMs do?

=> explain (done)

ln 774-776 I found this too terse. What is the n-gram count model? Why omit
the sentence environment?

=> explain (TODO Michael)

ln 778 What stimuli not including the preposition? Where are these
described?

=> explain (rephrased a bit)

4.4.2 If the words occur in the corpus, they presumably occur with their
article, so it's not immediately clear to me that the stimuli don't occur in
the corpus. Perhaps the unattested n-grams are the adj+N combination?

=> explain

ln 835 What does "strong semantic anomaly" mean and how is it checked for?

=> explain

ln 890 Threshold for what? (I couldn't quickly figure out what the 500
occurrence were *of*, nor what to compare to "above").

=> explain

ln 919ff I'm extremely skeptical of the claims about the sentence completion
task. In particular, no language model has information about "syntax,
lexical semantics, world knowledge, and pragmatics" beyond what can be
characterized in purely distributional terms --- i.e. what words share what
kind of distributional similarity with what other words. That will be a
partial reflection of part of speech (syntax-ish) and lexical semantics, but
it is no way "world knowledge". Furthermore, models don't "realize" anything
let alone "that [friend and mistress] are human beings".

=> rephrase and explain

ln 965 "somewhat deeper linguistic templates" seems like an overclaim.

=> explain

ln 990 Why didn't you include polysynthetic and agglutinative languages in
your testing? There are pretty good resources available for Inuktitut and
Turkish, respectively, for example.

=> discuss why

ln 991 "the common view that": This should come with citations. Places to
look are work on Construction Grammar (authors such as Chuck Fillmore and
Paul Kay) and also work by Ray Jackendoff.

=> we also have citations, discuss

ln 13-14 recently reached -> has recently reached
ln 096 as it goes -> as it gets
ln 149 model -> models?
ln 431 ad hoc doesn't need a hyphen
ln 531 can discover about -> can discover -or- can discover information
about
=>(done)
ln 622 I'm not sure what "the latter" is supposed to refer back to.
ln 720 the Universal Dependencies -> the German UD treebank
=>(done)
ln 996 capable to flexibly store -> capable of flexibly storing
=>(done)

=> fix these

**************
**************

B (a):

developers for their ongoing work.

1. Section 4.2 presents results on word segmentation. The paragraph
   starting on line 464 qualitatively investigates the errors made by
   the CNLM trained on Wikipedia test (note: it would be beneficial to
   state Wikipedia right at the beginning of the paragraph, rather
   than at its end). It would though be more interesting if this were
   a comparison between the Bayesian and the CNLM model, rather than
   just analyzing the CNLM.  Because, albeit the fact that "CNLM
   performance is comparable" (ref. to Table 4), a close look reveals
   that there is quiet a gap of the two models in terms of precision
   on inducing lexical word types. A comparative analysis would shed
   some light here, it might be that the LSTM gets frequent types
   right but misses other types, compared to the Bayesian method
   constructed with a lexical bias in mind.

=> this might go; introduce new qualitative analysis

2. For the first analysis (phonological classes induced by the output
   embeddings) results for German only are provided in Figure 1. The
   paper should include plots for all three languages, as there is no
   clear motivation why one was selected. There should be space to
   include all three plots.

=> this might go

3. What really surprised me is the bad performance of the vanilla RNN
   compared to the LSTM on the bigram acceptability judgment task
   (lines 382-383). This is in fact dramatic, as the model only needs
   to consider adjacent characters. At first it seems the model is
   underfit, but then the RNN performs reasonably well on other tasks,
   sometimes even being close to the LSTM (e.g., adj-gender agreement
   on Italian, Table 7) and perplexity scores are reasonable as
   well. Maybe a further discussion in light of training data
   properties and locality of the task might shed some light here (how
   long are the paragraphs the models are trained on?). Finally, what
   is also surprising is that the RNN does not improve with in-domain
   training data for the last task (sentence completion, see line 2 in
   Table 8). Why is the vanilla RNN not improving? Would it help to
   fine-tune on the in-domain data?

=> this might go

4. The paper does a great job in discussing related work. I though
   kept wondering about the difference with Kementchedjhieva & Lopez
   (2018). While overall results are in line (RNN-LMs do capture
   morphological properties), the paper is very brief on reporting an
   interesting divergence: "we could not replicate the result with our
   model" (on a single neuron tracking morpheme boundaries). It would
   be interesting to know if this is due to the different modeling
   setup (e.g., would this also hold for the model trained with white-space,
   footnote 6?) or what other reasons there could be at play.

=> discuss (done)

- Table 3: check F1 score for Italian (should be 59 rather than 60)
=> (this is gone)
- Presentation of results in Table 3 and 4: use of different decimal places.
=> (apparently solved)
- Colored figures are unreadable in b/w printing.
- line 936: in Figure 8 > in Table 8
=> done

=> fix 

**************
**************

C (c):

1. No usable datasets submitted.

=> correct this claim

I think this is an interesting area of inquiry.  The experiments in this
paper are extensive, but sometimes don't seem to fit the intent of the
authors and/or are not clearly explained.  The abstract really focuses on
the idea of removing spaces and still being able to recover words and
morphology, but the experiments veer away from that pretty quickly (starting
with experiment 5 below).

=> revise abstract and general claims as above

In general, there are too many experiments crammed into this paper,
and not enough explanation of the experimental set up, or careful
consideration of results.  This paper is right at the page limit, so I
think the authors should reconsider which experiments are most
telling, and move some of the extraneous ones to supplementary
material.

=> we will remove experiments and add explanation

I can't figure out from the TACL page if TACL allows supplementary
material, but in any case, there's too much in these 10 pages to cover
in the detail required for a reader to understand and be able to
reproduce any of these results.

=> unfortunatley not

1. Remove spaces, how does that affect perplexity/bits-per-char?
I'm not convinced that removing spaces is a good proxy to the word
segmentation problem infants and young children encounter, since they are
exposed to much simpler language (single words, very simple sentences).

=> We have bpc's results with spaces; sure not proxy, add caveat

2. Cluster characters by their embeddings.  Do the cluster represent
phonetics?
This experiment is not repeated (or results are not shown) for the RNN.  No
details are given for how the clustering was run (distance metric?) and the
cutoff for clusters appears to be chosen arbitrarily.

=> remove

3. Identify some acceptable and unacceptable bigrams in each language.
Train on data with both sets removed, and then test if the held out bigrams
are assigned probabilities that are consistent with the
acceptable/unacceptable categorization.
Here, I am very surprised that the RNN did so terribly, to the point where I
wonder if there is a bug in the analysis or code.  If there is no bug, I
think a better explanation for this behavior needs to be brought forward.
For example, perhaps the clustering as in Fig 1 would show that the
phonological categories are not learned by the RNN, which would help to
explain the lack of generalization we're seeing in this experiment (which
requires learning phonological categories).

=> remove

4a. Word segmentation
This experiment is not fully explained.  In particular the context PMI is
unclear to me here, and needs more explanation.  But somehow they are
creating features which they use to predict which characters start words

=> change to probing experiment

4b. A small little experiment with a LDA word segmenting algorithm is
included here, but so little detail is given that we can't draw much of a
conclusion.  It's also trained on a different corpus, so it sticks out a
bit.  Suggest this be put into a supplementary material section with more
details.

=> remove

4c. Error analysis
This is actually fairly interesting and I appreciate this qualitative
account

=> will have to replace

4d. Compare PMI to hierarchical distance
This experiment is really light on details and the accompanying figure 2 HAS
NO LABELS WHATSOEVER.  No axis labels and no legend labels!  There is only
one paragraph actually explaining this experiment, and it's not nearly
enough to understand the results.

=> discuss more in depth (TODO)

At this point we begin to veer off course, and the models seem to be trained
and/or tested on single words, which makes a bit of sense sometimes (e.g.
experiment 5 below which uses the models trained in previous sections) but
not always.

=> explain

5. Nouns vs verbs: can they be classified using the final hidden state of a
pre-trained model after reading the last char?
I don't speak German, but this sentence doesn't make any sense to me
"requiring that they end in -en (German) or -re (Italian) (so that models
canâ€™t rely on the affix for classification), " how would restricting the
suffix (en, re) also restrict the affix?  The baseline here is an
autoencoder LSTM trained on words in isolation.  This seems like a straw
man, if only shown words in isolation this model is missing much of the
context information that help the context-full LSTM tell verbs from nouns.

=> explain

6. Can the model detect number
Here I'm unclear what this has to do with the model trained on space-free
text.  The authors seem to be training on single words? "For the training
set, we randomly selected 15 singulars and plurals from each training
class."  The results show that the CNLM can't generalize to umlaut, but the
explanation is lacking (suffix vs internal root vowel change).  Why? is the
interesting question here.

=> discuss

There are many more experiments after this point, and the main themes of my
critiques are the same.  There is not enough information given to fully
understand these experiments (and thus replicating would be impossible).
The figures have NO labels.  There is no careful consideration of results.

=> add

line 242, the models were not trained until validation accuracy plateaued?
That does not seem standard.  How can we know if these models are fit to
compare against each other if we're not sure they're done training?

=> needs some discussion (still TODO)

The citations for the figures/tables are missing a lot of information.  It's
nice to not have to scan through the text to figure out what each figure is
showing, and many of the important details are left out of the
figures+captions (e.g. the acceptable/unacceptable order in Table 2, what
model is used to do the clustering in figure 1).  This is a little of
personal preference (which is why I include it here in the minor comments),
but it makes for an annoying first skim of the paper if you can't figure out
any of the figures without.  E.g. the caption for Table 5: "word class
accuracy with standard errors. ..."  For what task???

=> add

Tables with 9 cells, and 3 numbers per cell are pretty hard to parse e.g.
Table 3/4.  A bar graph with just F1 would be nice, unless the authors
actually think P/R make any contribution (they don't seem to talk about P/R
in any detail).

=> fix

Table 3 just gives P/R/F1, I don't think the claim on line 428 (classify
half the tokens correctly) can be inferred from P or R, rather, one needs
accuracy.

=> remove

Section 4.2 needs some subheaders, there's too much going on here and it's
hard to keep track of what the point of the current experiment is.

=> OK, if it doesn't go

A few tables/figures have STD or bootstrapped CI, but many do not. Would
like to see them consistently throughout

=> do or explain where we do it
=> We do it in those cases where there is a randomized train/test selection for a diagnostic classifier using small training data sets. For most of our numbers, standard errors/CIs would be very small, due to the large size of stimuli.

Line 616: "as above" there's a lot of stuff above at this point, please
refer to something more concrete

=> OK

Is table 8 really comparing results across corpora?  The top 3 models are
trained on wikipedia, and the bottom on Sherlock Holmes?  This is not a
sound comparison, not sure what we're supposed to take away from this
experiment

-> explain
