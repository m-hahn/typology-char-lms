\section{Introduction}
\label{sec:introduction}


Recurrent neural networks \cite[RNNs,][]{Elman:1990}, in particular
in their Long-Short-Term-Memory variant
\cite[LSTMs,][]{Hochreiter:Schmidhuber:1997}, are the current
workhorse of natural language processing. RNNs, often
pre-trained on the simple \emph{language modeling} objective of
predicting the next symbol in natural text, are a crucial
component of state-of-the-art architectures for machine
translation, natural language inference and text categorization
\cite{Goldberg:2017}.

RNNs are very general devices for sequence processing, assuming little
prior bias. Moreover, the simple prediction task they are trained on
in language modeling seems well-aligned with the core role prediction
plays in cognition \cite[e.g.,][]{Bar:2007,Clark:2016}. RNNs have thus
long attracted researchers
interested in language acquisition and processing. Their recent successes in
realistic large-scale tasks has strongly rekindled this interest
\cite[see, e.g.,][and references there]{Frank:etal:2013,Lau:etal:2017,Kirov:Cotterell:2018,McCoy:etal:2018,Pater:2018}.

The standard pre-processing pipeline of modern RNNs assumes that the
input has been tokenized into word units that are pre-stored in the
RNN vocabulary. This is a reasonable practical approach, but it makes
simulations less interesting from a linguistic point of view. First,
discovering words is one of the major challenges a learner faces, and
by pre-encoding them in the RNN we are facilitating its task in an
unnatural way (not even the staunchest nativists would take specific
word dictionaries to be part of our genetic code). Second, assuming a
unique tokenization into a finite number of discrete word units is in
any case problematic. The very notion of what counts as a word in
languages with a rich morphology is far from clear
\cite[e.g.,][]{Bickel:Zuniga:2017}, and, universally, mental lexicons
are probably organized into a not-necessarily-consistent hierarchy of
units at different levels: morphemes, words, compounds, constructions,
etc.~\cite[e.g.,][]{Goldberg:2005}. Indeed, it has been suggested that
the notion of word cannot even be meaningful defined on a
cross-linguistic level \cite{Haspelmath:2011}.

Motivated by these considerations, we study here RNNs that are trained
without any notion of word units in their input or in their
architecture. In particular, we train RNNs as \emph{character-level
  neural language models}
\cite[CNLMs,][]{Mikolov:etal:2011,Sutskever:etal:2011,DBLP:journals/corr/Graves13}. Moreover,
we remove whitespace from their input, so that, like children learning
a language, they don't have access to explicit cues to
wordhood.\footnote{We do not erase punctuation marks, reasoning that
  they have a similar function to prosodic cues in spoken language.}
This setup is almost as \emph{tabula rasa} as it goes. By using
unsegmented orthographic input (and assuming that, in the alphabetic
writing systems we work with, there is a reasonable correspondence
between letters and phonetic segments), we are only postulating that
the learner figured out how to segment the continuous speech stream
into phonological units, an ability children already possess few
months after birth
\cite[e.g.,][]{Maye:etal:2002,Kuhl:2004}.

After training the networks on the unsupervised character-level
language modeling task, we then task on a bank of morphological,
syntactic and semantic tests, in German, Italian and English. These
tasks intuitively require a model to have developed a latent ability
to parse characters into word-like units associated to morphological,
grammatical and broadly semantic features. We find that our RNNs pass
most of the tests, thus suggesting that the regularities they
encounter in their input at training time suffice to let them extract
and use lexical knowledge despite the lack of word-centric priors. In
a final experiment, we thus proceed to test more directly whether the
hidden activities of the RNNs is tracking information about word
boundaries, finding that indeed they do.  In the conclusion, we
discuss the broader implications and limits of our
simulations.\footnote{Upon publication, we will make our input data,
  test sets and pre-trained model available.}

% probe them with phonological, lexical,
% morphological, syntactic and semantic tests in English, German and
% Italian. Our results show that near-\emph{tabula-rasa} CNLMs acquire
% an impressive spectrum of linguistic knowledge at various levels.
% This in turn suggests that, given abundant input (large Wikipedia
% dumps), a learning device whose only prior architectural bias consists
% in the LSTM memory cell implicitly acquires a variety of linguistic
% rules that one would intuitively expect to require much more prior
% knowledge.
