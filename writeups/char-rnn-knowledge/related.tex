\section{Related work}
\label{sec:related}

\begin{itemize}
\item Character-based language models; emphasize:
  \begin{inparaenum}
  \item these models are not as good as word-based model
    perplexity-wise, but they have a more challenging task (they must
    explore a much wider possible-utterance space);
  \item lots of work on improving these models, in order to get better
    performance on morphologically rich languages etc: different from
    what we want to do, which is to probe the simplest CNLMs;
  \item Elman's might be most related in spirit, but just small toy
    experiment
  \end{inparaenum}
\item Work on discovering segmentation, in particular with CNLMs:
  clearly different
\item Work on understanding linguistic knowledge of NLMs: typically
  for word-based models
  \begin{itemize}
  \item Closer to us Alishahi et al.~2017, but they focus on a more
    complex architecture for grounded language learning, and on
    phonology
  \item Radford et al.~2017 probe CNLMs, but limited to sentement
    analysis
  \end{itemize}
\item What else?
\end{itemize}


 \cite{Gerz:etal:2018}: example of
``character-aware'' model, good to improve performance, but not what
we analyze here. Good paper to cite for earlier
references. \cite{Bojanowski:etal:2016}: might be mentioned for
showing that performance is worse than
word-RNN. \cite{Radford:etal:2017}: more precisely byte-based model, show good performance of CNLM as feature extractor, and does some qualitative analysis, in particular finding that sentiment is localistically encoded in a single cell. Early work showing that CLNMs can be used for language modeling, although they lag somewhat behind word models: \cite{Mikolov:etal:2011}, \cite{Sutskever:etal:2011} (the latter with some qualitative analysis),  \cite{Graves:2014} shows LSTM-based CLNMs to be only slightly worse than word-based one, and showed qualitatively they generate text with interesting properties, such as correctly balancing parentheses.

