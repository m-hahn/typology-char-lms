

\subsection{Semantics}
\label{sec:semantics}

Finally, we probe the CNLMs knowledge of semantics. We turn to
English, as for this language we can use the Microsoft Research
Sentence Completion task \cite{Zweig:Burges:2011}. The challenge
consists of sentences with a gap, and a 5-word multiple choice to fill
the gap. Picking the right word requires a mixture of syntax, lexical
semantics, world knowledge and pragmatics. For example, in \emph{``Was
  she his [
  \underline{client}|musings|discomfiture|choice|opportunity], his
  friend , or his mistress?}, the model should realize that the
missing word is coordinated with \emph{friend} and \emph{mistress},
and that the latter are human beings. We chose this challenge because
language models can be easily applied by calculating the likelihood of
all possible completions and selecting the one with the highest
likelihood. The domain of the task (Sherlock Holmes novels) is very
different from the Wikipedia data-set we are using; thus we
additionally trained our models on the training set provided for the
task, consisting of 19th century English novels.
%We both consider a fresh model trained on
%that data, and initializing it with the Wikipedia model.
% \footnote{For the WordNLM, the vocabulary consisted
% of the 50,000 most common words in the in-domain training set.}
%For comparison, we report results (KN5 from , LSTM from ) from previous work that were trained on the 19th century novels dataset (but the LSTM from that work had Glove embeddings). % \cite{zhang2016top} has a nice table if we want to report more

Results are shown in Figure~\ref{tab:msr-completion-results}.  The
models trained on Wikipedia perform poorly but above chance (20\%).  When trained on in-domain data, the LSTM CNLM outperforms many earlier word-level neural
models, and is only slightly below WordNLM. %, and approaches the best published results.
%, held by approaches developed for the completion task \cite{woods2016exploiting}.
% The best results I could find, https://github.com/ctr4si/sentence-completion, are much better than the best peer-reviewed published ones
The vanilla RNN is not successful even when trained in domain,
contrasting with \emph{word}-based vanilla RNNs, whose performance,
while below that of LSTMs, is much stronger. %  The WordNLM shows a
% slight boost over the
% CNLM.% \footnote{Results were significantly d
%egraded (50.1 \%) when using the Wikipedia vocabulary instead.}

% This experiment shows that a CNLM, trained without word boundaries, learns forms of semantic/world knowledge to a degree competitive with models trained on words.

\begin{table}[t]
  \begin{small}
    \begin{center}
      \begin{tabular}{l|l}
        % \multicolumn{1}{c}{}& Model \\
        LSTM 	    &      34.1/59.0 \\ % /59.2
        RNN  &     24.3/24.0 \\ % /27.1
        WordNLM & 37.1/63.3 \\ \hline % 50.1/52.4/
%        Random & 20.0 \\ \hline
        KN5   & 40.0 \\
        Word RNN & 45.0 \\
        Word LSTM & 56.0 \\ 
        LdTreeLSTM  & 60.67 \\	    \hline
        Skipgram + RNNs  & 58.9 \\
        \citet{woods2016exploiting} &  61.4 \\
        \citet{melamud2016context2vec} & 65.1 \\
      \end{tabular}
    \end{center}
  \end{small}
  \caption{\label{tab:msr-completion-results} Results on MSR Sentence Completion. For our models, we show accuracies for  Wikipedia/in-domain training. We compare with language models from prior work: Kneser-Ney 5-gram model \cite{Mikolov:2012}, Word RNN \cite{zweig2012computational}, Word LSTM and LdTreeLSTM \cite{zhang2016top}. We further report models incorporating distributional encodings of semantics: Skipgram+RNNs from \newcite{Mikolov:etal:2013b}, the PMI-based model of \citet{woods2016exploiting}, and the context-embedding based approach by \citet{melamud2016context2vec}.}
\end{table}



%, (3) the Wikipedia model posttrained on the in-domain data
% For the WordNLM, we additionally provide accuracy for a model with vocabulary derived from the in-domain training data. 
% \textbf{Explain what these are}


