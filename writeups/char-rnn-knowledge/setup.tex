\section{Experimental setup}
\label{sec:setup}

We downloaded full Wikipedia dumps for English, German, and Italian, and extracted plain text with WikiExtractor.\footnote{\url{https://github.com/attardi/wikiextractor}}
For each language, we randomly extracted testing and validation sections consisting of 50,000 paragraphs each, and used the remainder as training sets.
The training sets contained 16M (German), 9M (Italian), and 41M (English) paragraphs, corresponding to 819M (German), 463M (Italian), and 2,333M (English) words.
Order of paragraphs was shuffled for training; we did not attempt to split by sentences.
All characters were lower-cased.
For word segmentation and word-based language models, we tokenized and tagged all corpora using TreeTagger.\footnote{\url{http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/}}

For the vocabulary, we extracted the most frequent characters from
each corpus, setting thresholds so as to ensure that all characters
representing phonemes of the languages were included, resulting in vocabularies of 60 (English), 73 (German), and 59 (Italian) characters.
The vocabulary of the word-based models
included the most frequent 50,000 words per corpus.



For LSTM and RNN CNLMs, and for the word-level model, we specified a range of hyperparameter settings, and applied random search to find models that performed best on the validation set. \textbf{Report range of hyperparameters and best choices (possibly in appendix?).}
%We took X samples for the LSTM CNLM per language, Y for the RNN CNLM, and Z for the word-level LSTM.
We terminated training after 72 hours; none of the models had overfitted, as measured by performance on the validation set.

%<<<<<<< HEAD
Language modeling performance on the test partitions is shown in
Table~\ref{tab:lm-results}.  Direct comparison with the state of the art is hindered by
the fact that we train on text without whitespace: Whitespace is both
relatively easy to predict, and it makes predicting other
characters easier. Consequently, the fact that our character-level models are
below the state of the art is expected. For example, the best model of
\newcite{merity2018analysis} achieved 1.232 BPC on enwiki8
\cite{hutter2018}, a dataset also derived from English
Wikipedia. % (Hutter 2018). %, and 1.175 on a version of PTBenglish 0.85 german 0.9, italian 0.82
\textbf{I think that the Cotterell et al numbers might actually not be in BPC but in nat per character. We should ask them.}
On EuroParl data, \newcite{cotterell2018all} report 0.85 for English,
0.9 for German, and 0.82 for Italian. Still, our English BPC is
comparable, for example, to that reported by \newcite{Graves:2014} for
his static character-level LSTM trained on space-delimited Wikipedia data,
suggesting that we are working with reasonably high-performance language
models.
\footnote{Training our models on text with whitespace, without further hyperparameter tuning to adjust to that setting, resulted in cross-entropies of 0.91, 1.32 BPC (English), 0.89, 1.28 BPC (German), and 0.86, 1.24 BPC (Italian).}
The perplexity of the word-level model might not be comparable to
that of highly-optimized state-of-the-art architectures, but it is at the
expected level for a well-tuned vanilla LSTM language model. For
example, \newcite{Gulordava:etal:2018} report 51.9 perplexity for
their best LSTM trained on English Wikipedia data with the same vocabulary
size as ours.
%=======
%Performance on the test partitions is shown in Table~\ref{tab:lm-results}.
%Direct comparison with the state-of-the-art in character-based language modeling is hindered by the fact that we train on text without whitespace.
%The best models of \cite{merity2018analysis} achieved 1.232 BPC on enwiki8 \cite{hutter2018}, a dataset also derived from English Wikipedia. % (Hutter 2018). %, and 1.175 on a version of PTBenglish 0.85 german 0.9, italian 0.82
%On Europarl data, \cite{cotterell2018all} report 0.85 for English, 0.9 for German, and 0.82 for Italian. % I strongly suspect that their numbers are in nats, not bits, even though they call it BPC
%Our BPC values are higher, but this is expected given that we do not provide whitespace to the model: Whitespace is both relatively easy to predict, and it makes predicting other characters easier.\footnote{Refitting our models to data with whitespace, without retuning hyperparameters, yields ....}
%>>>>>>> 86b9fd533dc18bab83a010158126d7366aae3681

\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l|l|l|l}
      \multicolumn{1}{c}{}&\emph{LSTM}&\emph{RNN}&\emph{Word LSTM}\\
      \hline
	    English & 1.12 / 1.62 & 1.44 / 2.08 & 3.89 / 48.99  \\
	    German &  1.05 / 1.51 & 1.27 / 1.83 & 3.63 / 37.96   \\
	    Italian & 1.02 / 1.47 & 1.37 / 1.97 & 3.85 / 42.02  \\
    \end{tabular}
  \end{center}
  \caption{\label{tab:lm-results} Performance of language models. For CNLMs, we report cross-entropy and bits-per-character (BPC). For word-based models, we report cross-entropy and perplexity.}
\end{table}



