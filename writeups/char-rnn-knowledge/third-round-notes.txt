
"we do not erase punctuation marks"

Whitespace is certainly correlated with prosodic, phonetic and
phonological features. It seems difficult to me to argue
that it is less correlated than punctutation marks.

Could you just omit this footnote and simply say you're
deleting all whitespace?

> I'm not convinced, what do you think?

>> OK, I don't have strong feelings either way, I'm fine with ignoring
   the suggestion.

>>>> TODO

****************************************

"larger search space"

Why is the search space of character-level models larger?
Is this the case if you replace OOVs with a special token?
So that special token is then the reason for the reduced
search space?

> I still think this is true, for the reasons we discussed in Paris.

>> I tried to clarify as follows:  "This is
particularly encouraging in light of the fact that character-level
sentence prediction involves a much larger search space than
prediction at the word level, as a character-level model must make a
prediction after each character, rather than after each
word." Do you think this is clear?

>>> looks good to me!

>>>> We tried to clarify this as follows:  "This is
particularly encouraging in light of the fact that character-level
sentence prediction involves a much larger search space than
prediction at the word level, as a character-level model must make a
prediction after each character, rather than after each
word."


****************************************

related work

You should acknowledge other work that also has no explicit
lexicon. I'm specifically thinking of character-level
tagging, e.g.,

author  = {Dan Gillick and
 Cliff Brunk and
  Oriol Vinyals and
   Amarnag Subramanya},
     title = {Multilingual Language Processing From Bytes},

and of character-level work in machine learning, e.g.,

author  = {Jason Lee and
 Kyunghyun Cho and
  Thomas Hofmann},
    title = {Fully Character-Level Neural Machine
 Translation without Explicit
  Segmentation},

Colin Cherry, George Foster, Ankur Bapna, Orhan Firat,
Wolfgang Macherey:
Revisiting Character-Based Neural Machine Translation with
Capacity and Compression. EMNLP 2018: 4295-4305

True, these papers do not delete whitespace.

But your larger argument (no explicit representation of
words, no conventional notion of word) is applicable to this
work as well as it is to yours.

So if you write

"this suggests a radically different and possibly more
neurally plausible view ..."

then this prior work should be cited.

> Can you figure out where to put them? The bibtex keys are
> gillick2016multilingual, lee2017fully, cherry2018revisiting

>> I just added them exactly where Hinrich suggested we put them.

>>>> We added these citations at the suggested place.

****************************************

related work: other work on segmentation

As one of the reviewers pointed out, there is a literature
on word segmentation that should be cited and discussed,
especially Sharon Goldwater's work.

> can you look into this?

>> I added the following in related right after the comment about
   Kann's work, to futher extend the list of irrelevant work :):
   "There is also extensive work on segmentation of the linguistic
   signal that does not rely on neural methods, and is not directly
   relevant here, (e.g., Brent and Cartwright, 1996; Goldwater et al.,
   2009, and references there)."

>>>> We added the following: "There is also extensive work on segmentation of the linguistic
   signal that does not rely on neural methods, and is not directly
   relevant here, (e.g., Brent and Cartwright, 1996; Goldwater et al.,
   2009, and references there)."

****************************************

comparison with wordnlm

I find this comparison weird -- as you say yourself, you're
comparing apples and oranges here. Could you perhaps clearly
state what the purpose of this baseline is? Why don't you
run wordnlm on the same dataset as the other models?

> What do you think about this?

>> Honestly, I don't understand what it means that it not run on the
   same dataset as the other models. Do you know what he means? If
   not, I would just ignore.

>>> OK.

>>>> TODO

****************************************

compilation of dataset for "mit"

'sentence-initial "mit" is somewhat unnatural'

I don't see under what interpretation this could be a
correct statement. You could say with the same justification
(or lack thereof) that "dem sehr roten Baum" is unnatural
sentence-initially.

> I removed this, se also next point.

>> OK

>>>> We removed this.

---------------------------------------

"exclude the reading in which mit is a particle"

Can the particle "mit" precede an NP in a way that you get a
particle/preposition ambiguity? I didn't try very hard, but
it seems difficult to construct natural examples of this.

> I have added a citation and an example in a footnote, as I myself have found it nontrivial to construct such examples. (Such examples are mentioned in various dictionaries, of which I have cited one, so I'm confident this is not just my idiolect).

>> :)

>>>> We have added a citation and a footnote with a natural example representing the particle "mit": "An example of this unintended reading of mit is: Ich war mit der erste, der hier war. ‘I was one of the first who arrived here.’ In this context, dative ersten would be ungrammatical."

****************************************

dotted lines in Figure 1

You never explain these? Are they referring to this:

"We also created control stimuli where all words  up to
and including the preposition are removed"

In any case, please clarify!

> done

>> Thanks

>>>> We have clarified this in the text, in the paragraph discussing the Prepositional Case Subcategorization results.

****************************************

"tracking boundaries"

What do you mean when you say that tracking boundaries is
important? Is the main effect that certain character strings
are much more likely to occur after a boundary than after a
non-boundary (or vice versa)? So this then helps the model
in predictinng the next character?

> can you look into this?

>> I don't think I understood. Should we ignore?

>>> OK

>>>> TODO

****************************************

MINOR COMMENTS / TYPOS

- "current workhorse"

No longer true?  Transformers are as much a workhorse as
LSTMS now?

> should we rephrase?

>> Done. If they made us go through another pointless round of
   reviewing, probably even transformers would no longer be state of
   the art ;-)

>>>> We have rephrased this.

----------------------------------------------

- primacy *of* words

> done

>>>> Thanks, done.
----------------------------------------------

- treetagger: add citation for the original paper

>>>> Done.

----------------------------------------------

"as -al -o adjectives"
what are "-al -o adjectives"?

>>>> Fixed.

----------------------------------------------

- glosses are inconsistently typeset

mit `with'
vs
mit ``with''

>>>> Fixed.

----------------------------------------------

- hyphenation errors

You don't seem to use the correct hyphenation setup. Two
incorrectly hyphenated words I noticed:

whitespace, settling

>>>> Fixed.

----------------------------------------------

- encouraged -> encouraged us

>>>> Done.

----------------------------------------------

- hauptaufgabe haupt auf gabe : give english glosses

>>>> Done.

----------------------------------------------

- Why does the German model posit a boundary between
"s" and "ysteme" and between "dere" and "n"? (Or am I
reading the figure incorrectly?)

> For Systeme, we would need to know what came before. For deren, it's conceivable that dere-n seems like a reasonable segmentation (as derer, derem are also existing forms)

>> Can you add.

>>>> TODO regarding Systeme.
     Regarding deren, dere-n is a reasonable segmentation, as `derer' and `derem' are other existing forms of the same paradigm. we clarified by adding a sentence: "In the pronoun deren ‘whose’, the case suffix -n is separated."

----------------------------------------------

- Is the predicted segment "inseguitoa" or "nseguitoa"? it
looks like the latter.

> Plausibly yes. To tell whether that's reasonable, we would need to check what came before.

>> I'd ignore.

>>> OK

>>>> TODO

----------------------------------------------

- Did the absorption of the "case assigner" only happen for
Bau or also for other words?

> Should we look into this?

>> I feel like we're getting into a rabbit hole of things we could do, I'd ignore.

>>>> TODO

----------------------------------------------

- as the model reasonably segment
->
segments

>>>> Fixed.

----------------------------------------------

- I don't think "kon" and "kom" are reasonable
stems. "konn"/"komm" are (short vowels), but not "kon"/"kom"
(long vowels).

> I wonder whether the more reasonable interpretation is to think of kon- and kom- as the Latinate prefixes (kon-takt, kom-ponist), not as stems of verbs.

>> I added hedges to the current explanation (not that the model does get these strings also out of kommen and kennen.

>>>> TODO

----------------------------------------------

1. In several places, the paper makes what come across to me as overclaims
about cognitive plausibility. I think it would be stronger without these:

* ln025 "more cognitively reasonable task"

* ln102 "like children learning a language, they don’t have access to
explicit cues to wordhood.": Children learning their first language are
doing so in an extremely socially rich environment. Furthermore, they learn
words over time, such that known words can help with the acquisition and
even segmentation of later words. Furthermore, this claim about child
language acquisition is not backed up with citations to the relevant
literature, nor made precise: are you talking about word tokenization alone,
without any of the rest of learning of words (meanings)? At what stage, if
any, is that the task that children are approaching?

* ln112 "We believe that focusing on language modeling of an unsegmented
phoneme sequence, abstracting away from other complexities of a fully
realistic child language acquisition setup, is particularly instructive, in
order to study which linguistic structures naturally emerge." It may well be
instructive for studying CLNMs, but this makes it sound like you think it's
instructive for learning about actual child language acquisition. But why
would abstracting away from the details of the actual child's experience
make the model *more* instructive?

* ln1261 "This suggests a radically different and possibly more neurally
plausible view of the lexicon as implicitly encoded in a distributed memory,
that we intend to characterize more precisely and test in future work." This
suggests that the linguistic theories you cite higher in the paragraph are
making claims about the neural representation of the lexicon. HPSG, at
least, is emphatically not. I urge you to rephrase this --- or at the very
least make it clear what you claim to be different to. Also, the notion that
neural nets are actually reasonable models of human brains strikes me as an
enormous overclaim.

> can you look into these?

>> I think these are all deep philosophical points on which we have to
   agree to disagree, unless we want to turn our article into a
   50-page position paper.

>>>> TODO

--------------------------------------

2. I think the background/related work section should also review the non-NN
literature on unsupervised segmentation. I know at least Sharon Goldwater
has worked in this area:

Unsupervised Word Segmentation and Lexicon Discovery using Acoustic Word
Embeddings. Herman Kamper, Aren Jansen and Sharon Goldwater. IEEE
Transactions on Audio, Speech and Language Processing 24 (4), pp. 669–679.
2016.

> can you look into this? Probably we should reintroduce the citations to Sharon's earlier work.

>> I simply added the ref above to the same batch of "non-neural" segmentation work citations in related.tex

>>>> We have added this reference in the `Related Work' section, at the end of the `Character-based neural language models' section.

------------------------------------


ln418: Around here I found myself wondering whether the test words were OOV
for the CLNM --- I assume not, in the sense that the test words are in the
training data somewhere, just not high frequency enough to be in the
WordNLM's vocabulary? Perhaps this could be clarified.

> We do say that the words come from the Wikipedia training set (~line 385), so the reviewer's interpretation is correct. Do you think we need to be clearer, or can we rely on readers making this inference?

>> Unfortunately, the third-round main doesn't compile (missing figures), so I can't reconstruct what this refers to. I'd be in favour of clarifying, can you do it?

>>>> We have clarified this by adding the sentence: "Note that none of the words were OOV for the CNLM, as they all were taken from the Wikipedia training set."

-----------------------------------

ln613: "and not ending in -r, as the latter often reflect lemmatization
problems." Maybe this whole bit can be put in the footnote, since it doesn't
seem that relevant to the main text. Also "reflect lemmatization problems"
is still opaque to me, even with the footnote. Surely the test items involve
inflected forms and not lemmas?

>>>> Fixed.

-------------------------------------

ln686: "We study the preposition mit ‘with’, which selects a dative
object. We focus on mit, as it unambiguously requires a dative object,"
These two sentences seem fairly redundant to each other. Compress?

> I'm not sure how to make this better without losing clarity.

>> I think they are not redundant, and I would keep them.

>>>> TODO

----------------------------------------

ln783: "Words are the main carriers of lexical semantic information." I'm
not sure what this sentence is meant to convey. It sounds like a tautology
to me.

> I think it's fair to say that, in most linguistic theories, words are the main carriers of semantics: In formal models such as Montague's, words specify the components of meaning, and the rest is just how they are combined (The only counterpoint I can think of is the role of constructional meaning in Construction Grammar). Should we clarify in this direction?

>> I simply cut and added "word-based" to the next sentence, as I think the only point of that sentence here was to justify why we are interested in how well the CNLM performs this task.

>>>> TODO

-------------------------------------

ln1079: "are not directly comparable to the F1 results above" There are lots
of results above. Give a specific table number. Table 6?

>>>> Fixed.

-------------------------------------

ln1185: "The kom and kon cases are interesting, as the model reasonably
segment them as stems in forms of the verbs kommen ‘to come’ and kennen
‘to know’, respectively (e.g., kommt and konnte), but it also treats
them as pseudo-prefixes elsewhere (komponist ‘composer’, kontakt
‘contact’)." Nothing in the experiments shows that there is any
difference in the model's representation of stems v. prefixes. Where is this
claim coming from?

> This is related to the other comment about kom-/kon-.

>> I rephrased as "pseudo-affixes"

>>>> TODO

-----------------------------------

Style/typos:

* ln152 "primacy words" -> "primacy of words"
* ln158 "of words" -> "of the notion of word"
* ln179 "their greater generality": greater than what?
* ln222 "already Elman (1990) reported": this is an awkward/unusual use of
"already"
* ln265 "worth" -> "worthwhile"
* ln803 "twice more frequent" isn't idiomatic in English. Maybe you mean
"twice as frequent"?
* ln884 "took multiple provisions" -> "took multiple precautions"
* ln1138 "followed by von ‘of’ and genitive": I think 'and' should be
'or' here.
* ln1186 "reasonably segment" -> "reasonably segments"

>>>> Thanks, we fixed these.

------------------------------------------------------

1. The discussion of the segmentation results on the balanced setup (Table
7) states: "Moreover, in this more cogent setup, the single-unit LSTM
classifier outperforms the full-layer RNN classifier in all languages".
There seems to be a mistake here which needs to be fixed. This is not the
case for all languages. For Italian the single-unit LSTM does not outperform
the vanilla full RNN (i.e., single-unit LSTM 75.5 vs full RNN: 75.9).

>>>> Thanks, fixed.

----------------------------------------------

2. Style (Table 6+7) - presentation of results: I'd prefer to see
single/full for each model next to each other to ease reading, i.e.,
consider swapping the two middle columns (RNN single <> LSTM full).

> We have implemented this suggestion.

------------------------------------------------------


For each experiment, I would have expected to have a specific hypothesis
that is tested. If the goal is to investigate how whitespace-removal affects
the ability to learn different phenomena, then I would want to see a
character-level model with whitespace compared to a character-level model
without whitespace. If the goal is to investigate whether character-level
models can learn everything that word-level models can, I would have wanted
to see a character-level model with whitespace compared to a word-level
model. At the moment, the two phenomena are confounding each other and it is
difficult to draw conclusions. The only factor that is systematically
investigated is the choice of RNN vs LSTM, which is very well established in
previous work already.

> I think we're already quite explicit about our goal of testing a model that has no explicit cues to wordhood -- neither whitespace nor a vocabulary -- maybe you have an idea on how to make that even clearer?

>> I would ignore the comments of this reviewer, (s)he clearly disagrees with us, and I think at this point we must agree to disagree.

>>>> TODO

----------------------------------------------

The paper definitely has a lot of work put into it and a number of different
experiments. But after reading the whole paper several times, I am still
unsure of what the main take-away and novel contributions are. Readers like
me would probably benefit from more concretely spelling this out in the
introduction and the conclusion.

> can you think about this?

>> ditto

>>>> TODO

----------------------------------------------

At the moment, the main take-aways emphasised in sections 4.1-4.3 seem to be
that 1) language models still work as language models without whitespace,
and 2) LSTMs are better than RNNs. The first is fairly obvious and the
second has been demonstrated extensively in previous work.

> This is related to the question above. Do we need to more explicit about our goals?

>> ditto

>>>> TODO

----------------------------------------------

In section 4.2.1 the issue with calculating probabilities over different
length sequences is raised. Why not normalise the probability by the length
of the sequence?

> I don't think normalising solves the problem, as we care about the ovrall probability of a sequence, not a per-length average. For instance, a short correct word might easily be beaten by a long sequence where one character is very improbable but everything else is easy to predict from that -- which will swamp the one bad character when averaging over a long sequence.

>> ditto

>>>> We believe that normalizing probability by length would not solve the issue, as we care about the overall probability of a sequence, not a per-length average. If we compare sequences of different lengths by their normalized probabilities, then a short correct word might be assigned lower average probability than a long sequence where one character is very improbable but everything else is easy to predict from that. Restricting experimentation to sequences of equal length avoids this issue.



----------------------------------------------

In the introduction, the third paragraph argues how this work is important
because words are a very loose and ill-defined concept, whereas paragraph 5
argues why we should only evaluate this work on languages where words are
very clearly defined.

> I don't see a contradiction, but can you look into this?

>> ditto

>>>> TODO

----------------------------------------------

L370: properties of interests -> properties of interest

>>>> Done.

----------------------------------------------

L1132: "be reasonably be"

>>>> Done.

----------------------------------------------

